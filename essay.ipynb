{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "## Benchmarking State-of-the-Art Classifiers\n",
    "\n",
    "Oleksandra Kovalenko (578447)   \n",
    "Cosima Heymann (569413)  \n",
    "Sascha Geyer (546266)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sentiment](https://camo.githubusercontent.com/899f79e8a2d62fd642eba0791ff66d13d38e427901bfc3cd89c6f613311e1789/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f70726f78792f312a5f4a57314a614d704b5f6656476c64387064315f4a512e676966 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Research Overview\n",
    "3. Community Datasets Overview\n",
    "4. Experimental Setup\n",
    "       4.1 Datasets\n",
    "       4.2 Models\n",
    "       4.3 NLP Preprocessing Pipeline\n",
    "5. Results\n",
    "6. Conclusion\n",
    "7. Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: What is Sentiment Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The growth of user-generated content in web sites and social networks, just to mention a few: Yelp, Twitter, Amazon, Tripadvisor, Rottentomatoes and IMDB has led to an increasing power for expressing opinions. In recent years, the automatic extraction of opinions from a text has become an area of growing interest in Natural Language Processing (NLP). Online opinions have turned into a valuable asset since the fast spreading nature of online content. In order to analyze the massive amount of data, many NLP tasks are being used. In particular, Sentiment Analysis, also known as Opinion Mining (from now on: SA), became an increasingly growing task, whose goal it is to classify opinions and sentiments expressed in user-generated text. SA is on the rise due to the increased requirement of analyzing and structuring hidden information, which comes from user-generated content in the form of unstructured data (Ain, Ali, Riaz, Noureen, Kamran, Hayat & Rehman, 2017). It allows to detect the emotion and sentiment that an author of a text felt towards a described subject or entity. It is interesting in many fields and branches and helps solving various tasks, e.g.:\n",
    "\n",
    "- companies are able to measure the feedback about a product or service,\n",
    "- sociologists can look at people’s reaction about certain public events,\n",
    "- psychologists can study the general mind state of communities with regard to various issues, i.e. a depression detection model that is based on SA in micro-blog social networks (Wang, Zhang, Ji, Sun, Wu & Bao, 2013),\n",
    "- governments and political parties are able to correct their actions according to social approval or disapproval,\n",
    "- etc.\n",
    "\n",
    "The challenge is that sentiments are not always expressed explicitly and meanings can be hidden in the context. In these cases, additional word and language knowledge is necessary. Moreover, opinions may involve sarcasm and negations, which can be interpreted differently in various domains and contexts. Sentiment classification is rather easy for humans (Pang, Lee & Vaithyanathan, 2002), but manual review and analysis of texts is very time consuming and expensive. Due to this fact, automatic sentiment classifiers are selected instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis: Definition, Application & Classification \n",
    "\n",
    "Sentiment Analysis is an active research area in NLP that refers to the use of text analysis, statistical learning and often Machine Learning to extract subjective information in source materials such as user-generated texts from social networks, blogs, forums and product or service reviews.\n",
    "Selecting the basic emotions is a difficult task for a computer because of the variety of human emotions. Most of the authors in the NLP community agree on the classification proposed by Ekman, Friesen and Ellsworth (1982) that six basic emotions exist: anger, disgust, fear, joy, sadness and surprise. As such a division requires a complex processing and analysis of the input data, the majority of researchers and authors accept a simpler representation of sentiments according to their polarity (Pang & Lee, 2008). Kurosu (2015) defines sentiment polarity as follows: “The polarity of a sentiment is the point on the evaluation scale that corresponds to our positive or negative evaluation of the meaning of this sentiment.”. Sentiment polarity allows researchers to use a binary or ternary measurement, either positive, negative or neutral and therefore, simplifies the representation and management of the sentiment information. The granularity of SA can be either coarse-grained or fine-grained, where coarse-grained stands usually for a binary classification (positive, negative). On ther other hand, fine-grained can use five (or more) possible levels of granularity (high positive, low positive, neutral, low negative, high negative). \n",
    "Liu et al. (2015) presented three levels of SA: document level, sentence level and entity / aspect level. While document level studies the polarity of the whole text with respect to a single entity (e.g. a product), sentence level studies the polarity of single sentences, analyzing clauses and phrases for its sentiment. Contrary, entity / aspect level analyzes what people especially liked or disliked. An entity-aspect might be a single token and its polarity might be different from the overall polarity of the text (Liu et al., 2015).\n",
    "\n",
    "#####  Application\n",
    "SA can be applied in many areas. Below are a few application areas listed:\n",
    "\n",
    "- Social media monitoring,\n",
    "- Customer support / feedback,\n",
    "- Brand monitoring and reputation management,\n",
    "- Voice of customer,\n",
    "- Voice of employee,\n",
    "- Product analysis,\n",
    "- Market research and competitive research.\n",
    "\n",
    "##### Classification\n",
    "\n",
    "All methods used to solve sentiment classification fall into three main categories: lexicon-based, machine learning-based and hybrid approaches.\n",
    "\n",
    "In lexicon-based approaches, also known as knowledge-based methods, sentiment is seen as a function of keywords and is based on their count. The main task is the construction of sentiment word lexicons with the indicated class labels positive or negative. In some cases also with their intensiveness, which becomes important for a fine-grained classification.\n",
    "\n",
    "An alternative to the knowledge-based method is Machine Learning (ML), which is gaining more and more interest of researchers due to its adaptability and higher accuracy. Traditional Machine Learning methods were the dominant approach in SA (Pang et al., 2002) with the three main algorithms: Naïve Bayes (NB), Support Vector Machines (SVM) and Maximum Entropy (MaxEnt, in Statistics called: Logisitic Regression). Part of Machine Learning models are Deep Learning models (DL) and Transformer models.\n",
    "\n",
    "DL is an area of ML research that attempts to learn in multiple levels, corresponding to different levels of abstraction. Traditional ML relies on non-deep nets: composed of one input layer, one output layer and maximum one hidden layer inbetween. More than three layers (including input and output layer) qualifiy a net as “deep”.\n",
    "\n",
    "A Transformer is a type of neural network architecture developed by Vaswani et al. (2017). In short, this model architecture consists of a multi-head self-attention mechanism combined with an encoder-decoder structure. In a bit more detail, transformers work like this: first, the input embedding is multi-dimensional in the sense that it can process complete sentences and not a series of words one by one. Second, it has a powerful multi-headed attention mechanism that enables sentences to maintain context and relationships between words within a sentence. This attention analysis gets performed for each word several times to ensure adequate sampling. Lastly, it uses a feed forward neural network to normalize the results and provide a sentiment prediction. To learn more about the architecture of transformer models be sure to visit the the transformers library provided by Hugging Face [huggingface website](https://huggingface.co/docs/transformers/index) as this library gives you access to more than 32 pre-trained state-of-the-art (SOTA) models.\n",
    "\n",
    "The hybrid approach, also known as combined analysis or ensemble model, combines both knowledge-based and Machine Learning-based methods and thus, can lead to a superior performance. Researchers were attracted to explore the hybrid approach that collectively could exhibit the accuracy of a ML approach and the speed of a lexical approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section describes related works that exploits ML and DL approaches to solve SA tasks on different data sets and from different perspectives in the past 5 years. This review is conducted on the basis of numerous latest studies and researches in the field of SA. The first table presents several methods for English texts, whereas the second literature table presents papers for other languages like Greek, German or French. This field of research (SA for different languages) could be a topic for future studies. \n",
    "\n",
    "We compared the papers and models based on the following parameters:\n",
    "- paper name, \n",
    "- year of publication,\n",
    "- used datasets,\n",
    "- superior classification of used algorithm(s),\n",
    "- used algorithms,\n",
    "- used performance evaluation metrics and\n",
    "- where to find the paper.\n",
    "\n",
    "There are several papers out there that exploit the methods of lexicon-based models. In this eassy, we have only focused on ML-, DL-based and ensemble methods. If you want to get an overview of lexicon-based models, you could have a look at the paper from Vizcarra et al. (2021). \n",
    "\n",
    "In our literature review, we focussed on papers about ML and DL approaches (Singh, J., Singh, G., & Singh, R. (2017); Rustam, F., Ashraf, I., Mehmood, A., Ullah, S., & Choi, G. S. (2019); Ahuja, R., Chug, A., Kohli, S., Gupta, S., & Ahuja, P. (2019); Purchases, C. J. O. I., Stoyanova, L., & Wallace, W. (2019); da Silva, N. F. F., Coletta, L. F., Hruschka, E. R., & Hruschka Jr, E. R. (2016); Yi, S., & Liu, X. (2020); Ouyang, X., Zhou, P., Li, C. H., & Liu, L. (2015)). \n",
    "\n",
    "We also looked into hybrid approaches exploiting the power of ML approach and the stability from a lexicon-based approach (Gaye, B., Zhang, D., Wulamu, A. (2021); Novikova, A., Stupnikov, S. (2017); Alsayat, A. (2021); Araque, O., Corcuera-Platas, I., Sánchez-Rada, J. F., & Iglesias, C. A. (2017)). \n",
    "\n",
    "Last but not least, we also studied some papers about the latest trend and state-of-the-art in SA: Transformer models (Bacco, L., Cimino, A., Dell’Orletta, F., & Merone, M. (2021); Jiang, M., Wu, J., Shi, X., & Zhang, M. (2019); Wu, Z., Ying, C., Dai, X., Huang, S., & Chen, J. (2020))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Paper Name</strong>\n",
    "   </td>\n",
    "   <td><strong>Year of Publication</strong>\n",
    "   </td>\n",
    "   <td><strong>Dataset(s)</strong>\n",
    "   </td>\n",
    "   <td><strong>Classification</strong>\n",
    "   </td>\n",
    "   <td><strong>Algorithms</strong>\n",
    "   </td>\n",
    "   <td><strong>Performance Evaluation Criteria</strong>\n",
    "   </td>\n",
    "   <td><strong>Source</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Explainable Sentiment Analysis: A Hierarchical Transformer-Based Extractive Summarization Approach\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>IMDB\n",
    "   </td>\n",
    "   <td>Transformer \n",
    "   </td>\n",
    "   <td>Explainable Hierarchical Transformer (ExHiT),  Sentence Classification Combiner Model (SCC)\n",
    "   </td>\n",
    "   <td>accuracy\n",
    "   </td>\n",
    "   <td><a href=\"https://www.mdpi.com/2079-9292/10/18/2195/pdf\">https://www.mdpi.com/2079-9292/10/18/2195/pdf</a>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>A Tweet Sentiment Classification Approach Using a Hybrid Stacked Ensemble Technique\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>Sentiment140\n",
    "   </td>\n",
    "   <td>Hybrid \n",
    "   </td>\n",
    "   <td>stacked ensemble of three long short-term memory (LSTM) as base classifiers and logistic regression (LR) as a meta classifier\n",
    "   </td>\n",
    "   <td>accuracy, F1 \n",
    "   </td>\n",
    "   <td><a href=\"https://www.mdpi.com/2078-2489/12/9/374\">https://www.mdpi.com/2078-2489/12/9/374</a>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Optimization of sentiment analysis using machine learning classifiers\n",
    "   </td>\n",
    "   <td>2017\n",
    "   </td>\n",
    "   <td>3 manually compiled datasets; two of them are captured from Amazon and one dataset is assembled from IMDB movie reviews\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>Naïve Bayes, J48, BFTree and OneR\n",
    "   </td>\n",
    "   <td>accuracy, F-measure, correctly classified instances\n",
    "   </td>\n",
    "   <td><a href=\"https://doi.org/10.1186/S13673-017-0116-3\">https://doi.org/10.1186/S13673-017-0116-3</a>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Sentiment Analysis of Short Texts from Social Networks Using Sentiment Lexicons and Blending of Machine Learning Algorithms\n",
    "   </td>\n",
    "   <td>2017\n",
    "   </td>\n",
    "   <td>VKontakte social network posts\n",
    "   </td>\n",
    "   <td>Hybrid\n",
    "   </td>\n",
    "   <td>Logistic Regression, Random Forest Classifier, SVM, Gradient Boosting Classifier, KNeighbors Classifier, Multinomial Naive Bayes\n",
    "   </td>\n",
    "   <td>F1 \n",
    "   </td>\n",
    "   <td>http://ceur-ws.org/Vol-2268/paper21.pdf\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Tweets Classification on the Base of Sentiments for US Airline Companies\n",
    "   </td>\n",
    "   <td>2019\n",
    "   </td>\n",
    "   <td>Twitter US Airline Sentiment\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>Voting Classifier based on logistic regression (LR) and stochastic gradient descent classifier (SGDC) <strong>vs</strong> a variety of machine learning classifiers\n",
    "   </td>\n",
    "   <td>accuracy, F1\n",
    "   </td>\n",
    "   <td><a href=\"https://doi.org/10.3390/e21111078\">https://doi.org/10.3390/e21111078</a>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>The Impact of Features Extraction on the Sentiment Analysis\n",
    "   </td>\n",
    "   <td>2019\n",
    "   </td>\n",
    "   <td>Sentiment Strength Twitter Dataset\t\t\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>TFIDF vs N-gram on 6 ML algos (LR, SVM, Decision Tree, Random Forest, KNN, Naive Bayes)\n",
    "   </td>\n",
    "   <td>accuracy, F1 \n",
    "   </td>\n",
    "   <td>https://www.sciencedirect.com/science/article/pii/S1877050919306593\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>TOPIC MODELLING, SENTIMENT ANALSYS AND CLASSIFICATION OF SHORT-FORM TEXT\n",
    "   </td>\n",
    "   <td>2019\n",
    "   </td>\n",
    "   <td>data was obtained through\n",
    "Twitter and Facebook’s public APIs with Netlytic\n",
    "   </td>\n",
    "   <td>Lexicon-based, Machine Learning, Deep Learning\n",
    "   </td>\n",
    "   <td>LDA (Latent Dirichlet Allocation), \n",
    "LSA (Latent Semantic Allocation) vs LR, SVM and Naive Bayes\n",
    "   </td>\n",
    "   <td>technical performance (perplexity score and topic coherence score), ease of application, as well as proximity to human agent performance on the same problem\n",
    "   </td>\n",
    "   <td>https://local.cis.strath.ac.uk/wp/extras/msctheses/papers/strath_cis_publication_2733.pdf\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Using unsupervised information to improve semi-supervised tweet sentiment classification\n",
    "   </td>\n",
    "   <td>2016\n",
    "   </td>\n",
    "   <td>6 datasets: SemEval 2013, LiveJournal, SMS2013, Twitter2013, Twitter2014, Twitter Sarcasm 2014 \n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>semi-supervised C3E algorithmvs SVM\n",
    "   </td>\n",
    "   <td>F-Scores\n",
    "   </td>\n",
    "   <td>https://www.researchgate.net/publication/295244270_Using_unsupervised_information_to_improve_semi-supervised_tweet_sentiment_classification\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Improving Sentiment Analysis for Social Media Applications Using an Ensemble Deep Learning Language Model\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>3 datasets: own Twitter coronavirus hashtag dataset as well as public review datasets from Amazon and Yelp\n",
    "   </td>\n",
    "   <td>Hybrid\n",
    "   </td>\n",
    "   <td>customized deep learning model with an advanced word embedding technique and create a long short-term memory (LSTM)\n",
    "   </td>\n",
    "   <td>accuracy\n",
    "   </td>\n",
    "   <td>https://pubmed.ncbi.nlm.nih.gov/34660170/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Enhancing Deep Learning Sentiment Analysis with Ensemble Techniques in Social Applications\n",
    "   </td>\n",
    "   <td>2017\n",
    "   </td>\n",
    "   <td>7 datasets on movie reviews and microblogging \n",
    "   </td>\n",
    "   <td>Deep Learning, Hybrid\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>F1 \n",
    "   </td>\n",
    "   <td>https://www.researchgate.net/publication/313332224_Enhancing_Deep_Learning_Sentiment_Analysis_with_Ensemble_Techniques_in_Social_Applications\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Machine learning based customer sentiment analysis for recommending shoppers, shops based on customers’ review\n",
    "   </td>\n",
    "   <td>2020\n",
    "   </td>\n",
    "   <td>product data with customer reviews is collected from benchmark Unified computing system (UCS)\n",
    "   </td>\n",
    "   <td>Machine Learning \n",
    "   </td>\n",
    "   <td>Hybrid Recommendation System\n",
    "   </td>\n",
    "   <td>MAPE\n",
    "   </td>\n",
    "   <td>https://link.springer.com/article/10.1007/s40747-020-00155-2\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Sentiment Analysis Using Convolutional Neural Network\n",
    "   </td>\n",
    "   <td>2015\n",
    "   </td>\n",
    "   <td>IMDB\n",
    "   </td>\n",
    "   <td>Deep Learning\n",
    "   </td>\n",
    "   <td>RNN, LSTM, CNN,\n",
    "   </td>\n",
    "   <td>accuracy\n",
    "   </td>\n",
    "   <td>https://ieeexplore.ieee.org/abstract/document/7363395\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Transformer Based Memory Network for Sentiment Analysis of Web Comments\n",
    "   </td>\n",
    "   <td>2019\n",
    "   </td>\n",
    "   <td>2 datasets: Weibo, Semeval \n",
    "   </td>\n",
    "   <td>Transformer\n",
    "   </td>\n",
    "   <td>Transformer based memory network (TF-MN)\n",
    "   </td>\n",
    "   <td>accuracy, F1 \n",
    "   </td>\n",
    "   <td>https://www.researchgate.net/publication/337697651_Transformer_Based_Memory_Network_for_Sentiment_Analysis_of_Web_Comments\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Transformer-based Multi-Aspect Modeling for Multi-Aspect Multi-Sentiment Analysis\n",
    "   </td>\n",
    "   <td>2020\n",
    "   </td>\n",
    "   <td>MultiAspect Multi-Sentiment (MAMS) dataset\n",
    "   </td>\n",
    "   <td>Transformer\n",
    "   </td>\n",
    "   <td>RoBERTa-Transformer-based\n",
    "Multi-aspect Modeling method (TMM)\n",
    "   </td>\n",
    "   <td>accuracy, F1\n",
    "   </td>\n",
    "   <td>https://arxiv.org/abs/2011.00476\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<strong>Table 1. Literature Review for English Datasets</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SA in languages other than English became a popular research area in the last 1-2 years. These papers also featured only ML and DL based methods. We studied the following papers: Elfaik, H. (2021); Alexandridis, G., Varlamis, I., Korovesis, K., Caridakis, G., & Tsantilas, P. (2021); Flender, M., & Gips, C. (2017); Tellez, E. S., Miranda-Jiménez, S., Graff, M., Moctezuma, D., Siordia, O. S., & Villaseñor, E. A. (2017); Carosia, A. E. O., Coelho, G. P., & Silva, A. E. A. (2020); Rhouati, A., Berrich, J., Belkasmi, M. G., & Bouchentouf, T. (2018).\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Language</strong>\n",
    "   </td>\n",
    "   <td><strong>Paper Name</strong>\n",
    "   </td>\n",
    "   <td><strong>Year of Publication</strong>\n",
    "   </td>\n",
    "   <td><strong>Dataset(s)</strong>\n",
    "   </td>\n",
    "   <td><strong>Classification</strong>\n",
    "   </td>\n",
    "   <td><strong>Algorithms</strong>\n",
    "   </td>\n",
    "   <td><strong>Performance Evaluation Criteria</strong>\n",
    "   </td>\n",
    "   <td><strong>Source</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Arabic\n",
    "   </td>\n",
    "   <td>Deep Bidirectional LSTM Network Learning-Based Sentiment Analysis for Arabic Text\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>6 benchmark sentiment analysis datasets\n",
    "   </td>\n",
    "   <td>Deep Learning\n",
    "   </td>\n",
    "   <td>Bidirectional LSTM Network (BiLSTM)\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>https://www.degruyter.com/document/doi/10.1515/jisys-2020-0021/html\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Greek\n",
    "   </td>\n",
    "   <td>A Survey on Sentiment Analysis and Opinion Mining in Greek Social Media\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>self-collected and annotated Greek Social Media Texts \n",
    "   </td>\n",
    "   <td>Deep Learning\n",
    "   </td>\n",
    "   <td>PaloBert, GreekBERT\n",
    "   </td>\n",
    "   <td>accuracy, F1\n",
    "   </td>\n",
    "   <td>https://doi.org/10.3390/info12080331\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>German\n",
    "   </td>\n",
    "   <td>Sentiment analysis of a German Twitter-Corpus\n",
    "   </td>\n",
    "   <td>2017\n",
    "   </td>\n",
    "   <td>German tweets from a bigger dataset\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>Multinomial NB,  LinearSVC, Decision Tree Classifier, Maxent Classifier\n",
    "   </td>\n",
    "   <td>accuracy, F1\n",
    "   </td>\n",
    "   <td>http://ceur-ws.org/Vol-1917/paper06.pdf\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Spanish\n",
    "   </td>\n",
    "   <td>A case study of Spanish text transformations for twitter sentiment analysis\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>2 Spanish datasets\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>SVM\n",
    "   </td>\n",
    "   <td>accuracy, computing time\n",
    "   </td>\n",
    "   <td>https://www.sciencedirect.com/science/article/abs/pii/S0957417417302312?via%3Dihub\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>(Brazilian) Portuguese\n",
    "   </td>\n",
    "   <td>Analyzing the Brazilian Financial Market Through Portuguese Sentiment Analysis in Social Media\n",
    "   </td>\n",
    "   <td>2018\n",
    "   </td>\n",
    "   <td>self annotated Twitter dataset on financial market\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>Naive Bayes, Support Vector Machines, Maximum Entropy and Multilayer Perceptron\n",
    "   </td>\n",
    "   <td>accuracy\n",
    "   </td>\n",
    "   <td>https://www.researchgate.net/profile/Arthur-Carosia/publication/336933355_Analyzing_the_Brazilian_Financial_Market_through_Portuguese_Sentiment_Analysis_in_Social_Media/links/5e67edc24585153fb3d5b305/Analyzing-the-Brazilian-Financial-Market-through-Portuguese-Sentiment-Analysis-in-Social-Media.pdf\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>French\n",
    "   </td>\n",
    "   <td>Sentiment Analysis of French Tweets based on Subjective Lexicon Approach: Evaluation of the use of OpenNLP and CoreNLP Tools\n",
    "   </td>\n",
    "   <td>2018\n",
    "   </td>\n",
    "   <td>French tweets using \"Public Opinion Knowledge (POK)\" platform\n",
    "   </td>\n",
    "   <td>Lexicon based in comparison to Machine Learning\n",
    "   </td>\n",
    "   <td>OpenNLP, CoreNLP, dependency analysis implemented by CoreNLP\n",
    "   </td>\n",
    "   <td>F-Scores\n",
    "   </td>\n",
    "   <td>https://www.researchgate.net/publication/326514882_Sentiment_Analysis_of_French_Tweets_based_on_Subjective_Lexicon_Approach_Evaluation_of_the_use_of_OpenNLP_and_CoreNLP_Tools\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<strong>Table 2. Literature Review for other Languages</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may ask what the future holds for Sentiment Analysis. It is pretty clear to us that there is still lots of room to improve the performance measures of transformer models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Community Datasets Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an overview of community datasets for the use of SA that are publicly available. We have outlined them with the following features: \n",
    "- name,\n",
    "- platform,\n",
    "- domain,\n",
    "- size, \n",
    "- evaluation, \n",
    "- language,\n",
    "- source.\n",
    "\n",
    "This list is by far not extensive and lacks for example datasets in different languages other than English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Name</strong>\n",
    "   </td>\n",
    "   <td><strong>Platform</strong>\n",
    "   </td>\n",
    "   <td><strong>Domain</strong>\n",
    "   </td>\n",
    "   <td><strong>Size</strong>\n",
    "   </td>\n",
    "   <td><strong>Evaluation (binary or more)</strong>\n",
    "   </td>\n",
    "   <td><strong>Language</strong>\n",
    "   </td>\n",
    "   <td><strong>Source</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Twitter US Airline Sentiment\n",
    "   </td>\n",
    "   <td>Twitter \n",
    "   </td>\n",
    "   <td>US Airline user experiences\n",
    "   </td>\n",
    "   <td>3.42 MB\n",
    "   </td>\n",
    "   <td>ternary = positive, negative, neutral\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Sentiment140\n",
    "   </td>\n",
    "   <td>Twitter\n",
    "   </td>\n",
    "   <td>user responses to different products, brands, or topics\n",
    "   </td>\n",
    "   <td>228 MB Training (1.600.000) \n",
    "   </td>\n",
    "   <td>0 = negative, \n",
    "2 = neutral, 4 = positive\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>http://help.sentiment140.com/for-students\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Stanford Sentiment Treebank\n",
    "   </td>\n",
    "   <td>Rotten Tomatoes\n",
    "   </td>\n",
    "   <td>movie reviews\n",
    "   </td>\n",
    "   <td>10.000\n",
    "   </td>\n",
    "   <td>1-25 (25: most positive)\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://nlp.stanford.edu/sentiment/code.html\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Large IMDB Movie Reviews\n",
    "   </td>\n",
    "   <td>IMDB\n",
    "   </td>\n",
    "   <td>movie reviews\n",
    "   </td>\n",
    "   <td>25.000 training, 25.000 test\n",
    "   </td>\n",
    "   <td>binary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Polarity v2.0\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>movie reviews\n",
    "   </td>\n",
    "   <td>3MB (1000 positive and 1000 negative processed reviews)\n",
    "   </td>\n",
    "   <td>binary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Paper Reviews\n",
    "   </td>\n",
    "   <td>conference of computing\n",
    "   </td>\n",
    "   <td>user’s opinion about a paper\n",
    "   </td>\n",
    "   <td>405\n",
    "   </td>\n",
    "   <td>-2: very negative,\n",
    "-1: negative,\n",
    "0: neutral,\n",
    "1: positive,\n",
    "2: very positive\n",
    "   </td>\n",
    "   <td>English, Spanish\n",
    "   </td>\n",
    "   <td>https://archive.ics.uci.edu/ml/datasets/Paper+Reviews\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Multi-Domain Sentiment Dataset\n",
    "   </td>\n",
    "   <td>Amazon\n",
    "   </td>\n",
    "   <td>reviews of amazon products\n",
    "   </td>\n",
    "   <td>unprocessed: 1.9 GB, processed: 19 MB\n",
    "   </td>\n",
    "   <td>reviews contain ratings from 1 to 5 stars (can be converted to binary)\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://www.cs.jhu.edu/~mdredze/datasets/sentiment/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Opin-Rank Review Dataset\n",
    "   </td>\n",
    "   <td>Tripadvisor, Edmunds\n",
    "   </td>\n",
    "   <td>hotel, car reviews\n",
    "   </td>\n",
    "   <td>300.000\n",
    "   </td>\n",
    "   <td>ratings that can be turned into binary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://archive.ics.uci.edu/ml/datasets/opinrank+review+dataset\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Sentiment Lexicons For 81 Languages\n",
    "   </td>\n",
    "   <td>-\n",
    "   </td>\n",
    "   <td>-\n",
    "   </td>\n",
    "   <td>2 text files per language\n",
    "   </td>\n",
    "   <td>binary\n",
    "   </td>\n",
    "   <td>81 languages: Afrikaans to Yiddisch\n",
    "   </td>\n",
    "   <td>https://sites.google.com/site/datascienceslab/projects/multilingualsentiment\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Lexicoder\n",
    "   </td>\n",
    "   <td>-\n",
    "   </td>\n",
    "   <td>-\n",
    "   </td>\n",
    "   <td>2,858 negative sentiment words and 1,709 positive sentiment words\n",
    "   </td>\n",
    "   <td>binary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>http://www.snsoroka.com/data-lexicoder/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>DynaSent\n",
    "   </td>\n",
    "   <td>Dynabench\n",
    "   </td>\n",
    "   <td>naturally occurring sentences with sentences created using the open-source Dynabench Platform\n",
    "   </td>\n",
    "   <td>121.634 sentences\n",
    "   </td>\n",
    "   <td>ternary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://github.com/cgpotts/dynasent\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Amazon Fine Foods\n",
    "   </td>\n",
    "   <td>Amazon\n",
    "   </td>\n",
    "   <td>product reviews\n",
    "   </td>\n",
    "   <td>5.000.000 reviews\n",
    "   </td>\n",
    "   <td>ratings that can be turned into binary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://snap.stanford.edu/data/web-FineFoods.html\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Germeval2017\n",
    "   </td>\n",
    "   <td>Social Media \n",
    "   </td>\n",
    "   <td>messages from various social media and web sources\n",
    "   </td>\n",
    "   <td>22.000 messages \n",
    "   </td>\n",
    "   <td>ternary\n",
    "   </td>\n",
    "   <td>German\n",
    "   </td>\n",
    "   <td>https://sites.google.com/view/germeval2017-absa/data\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Yelp_polarity_reviews\n",
    "   </td>\n",
    "   <td>Yelp\n",
    "   </td>\n",
    "   <td>business reviews\n",
    "   </td>\n",
    "   <td>600.000 reviews for training, 38.000 for testing\n",
    "   </td>\n",
    "   <td>binary (1 - bad, 2 - good)\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://www.kaggle.com/irustandi/yelp-review-polarity\">https://www.kaggle.com/irustandi/yelp-review-polarity\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Financial PhraseBank\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>financial news (rated as pos/neg/neutral) for investor\n",
    "   </td>\n",
    "   <td>4840 (4 configurations available (size depends on the level of agreement of annotators))\n",
    "   </td>\n",
    "   <td>ternary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://github.com/huggingface/datasets/tree/master/datasets/financial_phrasebank\">https://github.com/huggingface/datasets/tree/master/datasets/financial_phrasebank\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>The SigmaLaw- Aspect-Based-SA dataset\n",
    "   </td>\n",
    "   <td>Court cases\n",
    "   </td>\n",
    "   <td>Legal opinion texts\n",
    "   </td>\n",
    "   <td>2.000 sentences\n",
    "   </td>\n",
    "   <td>ternary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://osf.io/efrqt/\">https://osf.io/efrqt/\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<strong>Table 3. Overview of Community Datasets</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to compare selected DL and Transformer models with the performance of a traditional ML model (our baseline) on a document-level sentiment analysis task. For this purpose we selected two datasets from different domains: \n",
    "\n",
    "- Large IMDB Movie reviews\n",
    "\n",
    "This dataset from Stanford researchers consists of 50.000 polarized, binary labeled reviews. It is important to note that the data was originally split by researchers 50/50 for training and test purposes, so the dataset is balanced. In our experiment, we used the joined dataset and performed the train-test split by ourselves aiming to leave more data for training.\n",
    "\n",
    "\n",
    "- Sentiment 140\n",
    "\n",
    "Sentiment140 was created by Alec Go, Richa Bhayani, and Lei Huang, who were Computer Science graduate students at Stanford University. The dataset allows you to discover the sentiment of a brand, product or topic on Twitter. We use the version of the dataset which consists of 1.6 mln. tweets with equally distributed labels (50% positive, 50% negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model\n",
    "\n",
    "This experiment used the popular and simple ML algorithm Logistic Regression as a baseline algorithm. Logistic Regression is a statistical technique capable of predicting a binary outcome. Using the built-in functions of scikit-learn, the Logistic Regression was very easy to build. After loading the respective dataset, the code had to re-create all the words from the preprocessed data set to build an index, which translates all lists of word-indices to strings and then used Term Frequence - Inverse Document Frequency (TF-IDF) as text representation. TF-IDF is a statistical measure used to evaluate how important a word is in a document. First, it computes the Term Frequence (TF) for each review, the Inverse Document Frequency (IDF) using each review and finally, the TF-IDF for each review. It transforms on the Test data which computes the TF for each review, then the TF-IDF for each review using the IDF from the Training data. Finally, the model was fit to classify the sentiment of the movie reviews and tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DL model\n",
    "\n",
    "As Long Short Term Memory networks (LSTM) are the most often used DL models for SA (Ligthart et. al., 2021), we decided to test them in our experiment. Our vanilla LSTM architecture consists of an embedding, LSTM, and a fully connected layer. In between we use two dropout layers to prevent overfitting.\n",
    "We found the blog of Christopher Olah particulary helpful and suggest to check it out for gaining a first understanding of how LSTMs work: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "According to the findings of Ligthart et al. (2021), convolutional neural networks (CNN) are the second most often used DL models for SA. Recent research also indicates that a hybrid CNN & LSTM model can outperform traditional ML algorithms on SA tasks (e.g. Jain et al.,2021; Yadav & Vishwakarma, 2020). Our proposed architecture consists of an embedding layer, a convolution layer which receives the input from the embedding layer, a pooling layer, the output of which is fed into the LSTM layer, two fully connected, as well as dropout layers.\n",
    "\n",
    "We tokenize and pad the data using Keras built-in functions in order to bring all reviews / tweets in the suitable format. An Embedding layer is necessary for dimensionality reduction which is achieved by converting each word into a vector of defined length. Similar words have similar embeddings. We tried out two approaches: \n",
    "- training embeddings from scratch on the vocabularies from our respective datasets and \n",
    "- using pre-trained embeddings (transfer learning). We used GloVe (Global Vectors for Word Representation) embeddings from Stanford researchers (Pennington et al., 2014) trained on 1) Wikipedia texts and 2) Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Short name|Training data|Number of tokens|Vocabulary|Dimensionality|\n",
    "|------|------|------|------|------|\n",
    "|Glove Wiki|combination of Wikipedia 2014 & Gigaword5|6B tokens| 400 thousand |50d, 100d, 200d, & 300d vectors|\n",
    "|Glove Tweets |2B tweets | 27B tokens| 1.2 million |25d, 50d, 100d, & 200d vectors|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe embeddings can be easily downloaded from the respective Stanford website which takes around 5-10 minutes depending on your internet connection. We provide the links below in the code. There also exist GloVe embeddings pretrained on other data which we don't use in our essay. They are also publicly available on the project web page: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT is a Transformer model based on the BERT architecture which is smaller, faster, lighter and cheaper to pre-train. Knowledge distillation is performed during the pre-training phase in order to reduce the size of a BERT model by 40%. To leverage the inductive biases learned by larger models during pre-training, the authors introduce a triple loss combining language modeling, distillation and cosine-distance losses (Sanh, Debut, Chaumond & Wolf, 2019).\n",
    "\n",
    "#### GPU usage in experiment\n",
    "\n",
    "All experiments were run using NVIDIA TESLA P100 GPU from Kaggle. The usage is free of charge and limited to 30-40 hours/week (see more here: https://www.kaggle.com/docs/efficient-gpu-usage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 NLP Preprocessing Pipeline\n",
    "\n",
    "We compare the performance of the selected models on raw and preprocessed datasets. For this task we setup a Preprocessor Class which runs a NLP Pipeline to take care of all necessary preprocessing tasks such as lemmatization, stopword removal and text cleaning with regular expressions. In addition, the Preprocessor also factorizes the label to obtain a binary encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Preprocessor import Preprocessor\n",
    "from LogisticRegression import LogisticRegression\n",
    "from LSTM import LSTM\n",
    "from DistilBert import DistilBert\n",
    "\n",
    "imdb_df = pd.read_csv(\"./data/IMDB.csv\", names=[\"text\", \"sentiment\"])\n",
    "sentiment140_df = pd.read_csv(\"./data/Sentiment140.csv\", header=None, index_col=False, encoding='latin-1', usecols=[0,5], names=[\"sentiment\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_sentiment140_df = sentiment140_df.tail(5000)\n",
    "#small_sentiment140_df = small_sentiment140_df.append(sentiment140_df.head(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"imdb\": {\n",
    "         \"preprocessor\": {\n",
    "            \"name\": \"imdb\",\n",
    "            \"df\": imdb_df,\n",
    "            \"cache\": True,\n",
    "            \"test_size\": 0.3,\n",
    "             \"random_state\": 42,\n",
    "        },\n",
    "        \"logistic_regression\": {\n",
    "            \"ngram_range\": (1,1),\n",
    "            \"random_state\": 42,\n",
    "            \"max_iter\": 100,\n",
    "        },\n",
    "        \"LSTM\": {\n",
    "            \"lstm_units\": 80,\n",
    "            \"batch_size\": 256,\n",
    "            \"dropout_rate\": 0.1,\n",
    "            \"activation\": \"sigmoid\",\n",
    "            \"epochs\": 3,\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "    },\n",
    "    \"sentiment140\": {\n",
    "       \"preprocessor\": {\n",
    "            \"name\": \"sentiment140\",\n",
    "            \"df\": sentiment140_df,\n",
    "            \"cache\": True\n",
    "        },\n",
    "        \"logistic_regression\": {\n",
    "            \"ngram_range\": (1,3),\n",
    "            \"random_state\": 42,\n",
    "            \"max_iter\": 500,\n",
    "        },\n",
    "        \"LSTM\": {\n",
    "            \"lstm_units\": 27,\n",
    "            \"batch_size\": 4096,\n",
    "            \"dropout_rate\": 0.1,\n",
    "            \"activation\": \"sigmoid\",\n",
    "            \"epochs\": 3,\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_models(config):\n",
    "    # -------------- Preprocessor -------------- #\n",
    "    preprocessor = Preprocessor(**config['preprocessor'])\n",
    "    Xy = preprocessor.run()\n",
    "    Xy_train_test_dict = preprocessor.split(Xy)\n",
    "    \n",
    "    # ----------- LogisticRegression ----------- #\n",
    "    lr_model = LogisticRegression(**config['logistic_regression'])\n",
    "    lr_model.fit(Xy_train_test_dict)\n",
    "    \n",
    "    # ------------------ LSTM ------------------ #\n",
    "    lstm_model = LSTM(**config['LSTM'])\n",
    "    lstm_model.fit(Xy_train_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read imdb_preprocessed.parquet.gzip from cache...\n",
      "Successfully read imdb_preprocessed.parquet.gzip into memory.\n",
      "Logistic Regression validation accuracy: 0.8904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 16:25:44.357271: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "117/117 [==============================] - 91s 763ms/step - loss: 0.5771 - accuracy: 0.7181 - val_loss: 0.3384 - val_accuracy: 0.8592\n",
      "Epoch 2/3\n",
      "117/117 [==============================] - 92s 787ms/step - loss: 0.2778 - accuracy: 0.8880 - val_loss: 0.2862 - val_accuracy: 0.8859\n",
      "Epoch 3/3\n",
      "117/117 [==============================] - 93s 793ms/step - loss: 0.2077 - accuracy: 0.9219 - val_loss: 0.2735 - val_accuracy: 0.8880\n",
      "469/469 [==============================] - 25s 53ms/step - loss: 0.2912 - accuracy: 0.8811\n",
      "LSTM validation accuracy: 88.11%\n"
     ]
    }
   ],
   "source": [
    "run_models(configs[\"imdb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read sentiment140_preprocessed.parquet.gzip from cache...\n",
      "Successfully read sentiment140_preprocessed.parquet.gzip into memory.\n",
      "Logistic Regression validation accuracy: 0.7786958333333334\n",
      "Epoch 1/3\n",
      "233/233 [==============================] - 813s 3s/step - loss: 0.5461 - accuracy: 0.7307 - val_loss: 0.4855 - val_accuracy: 0.7669\n",
      "Epoch 2/3\n",
      "233/233 [==============================] - 822s 4s/step - loss: 0.4816 - accuracy: 0.7690 - val_loss: 0.4773 - val_accuracy: 0.7692\n",
      "Epoch 3/3\n",
      "233/233 [==============================] - 819s 4s/step - loss: 0.4741 - accuracy: 0.7721 - val_loss: 0.4744 - val_accuracy: 0.7695\n",
      "15000/15000 [==============================] - 355s 24ms/step - loss: 0.4759 - accuracy: 0.7698\n",
      "LSTM validation accuracy: 76.98%\n"
     ]
    }
   ],
   "source": [
    "run_models(configs[\"sentiment140\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_preprocessed = pd.read_parquet(\"/cache/imdb_preprocessed.parquet.gzip\")\n",
    "sent140_preprocessed = pd.read_parquet(\"/cache/sentiment140_preprocessed.parquet.gzip\")\n",
    "\n",
    "sets_raw_imdb=preprocessor.split(imdb_df,0.3,42)\n",
    "sets_preprocessed_imdb=preprocessor.split(imdb_preprocessed,0.3,42)\n",
    "sets_raw_sent140=preprocessor.split(sentiment140_df,0.3,42)\n",
    "sets_preprocessed_sent140=preprocessor.split(sent140_preprocessed,0.3,42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model on raw datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM on raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM_new import LSTM\n",
    "from CNN_LSTM import CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "\n",
    "# Allow memory growth for the GPU\n",
    "physical_devices = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "tensorflow.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import embeddings\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "\n",
    "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "!unzip glove.twitter.27B.zip\n",
    "\n",
    "GLOVE_EMB_WIKI_50 = './glove.6B.50d.txt'\n",
    "GLOVE_EMB_WIKI_100 = './glove.6B.100d.txt'\n",
    "GLOVE_EMB_WIKI_200 = './glove.6B.200d.txt'\n",
    "GLOVE_EMB_WIKI_300 = './glove.6B.300d.txt'\n",
    "\n",
    "GLOVE_EMB_TWI_25 = '/glove.twitter.27B.25d.txt'\n",
    "GLOVE_EMB_TWI_50 = '/glove.twitter.27B.50d.txt'\n",
    "GLOVE_EMB_TWI_100 = '/glove.twitter.27B.100d.txt'\n",
    "GLOVE_EMB_TWI_200 = '/glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM on IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T00:16:01.737432Z",
     "iopub.status.busy": "2022-02-12T00:16:01.737160Z",
     "iopub.status.idle": "2022-02-12T00:32:05.601504Z",
     "shell.execute_reply": "2022-02-12T00:32:05.600783Z",
     "shell.execute_reply.started": "2022-02-12T00:16:01.737403Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of the dataset is :  105600\n",
      "2493\n",
      "Data padded!\n",
      "Found 400000 word vectors.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 2493, 300)         31680300  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2493, 300)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 31,847,229\n",
      "Trainable params: 166,929\n",
      "Non-trainable params: 31,680,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "469/469 [==============================] - 20s 43ms/step - loss: 0.4006 - accuracy: 0.9031\n",
      "LSTM Accuracy: 90.31%\n"
     ]
    }
   ],
   "source": [
    "lstm_model.fit_lstm(sets_raw_imdb,epochs=20, batch_size=128,embedding_dim=300,embeddings_name=GLOVE_EMB_WIKI_300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM on Sentiment140 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T01:06:23.983651Z",
     "iopub.status.busy": "2022-02-12T01:06:23.983339Z",
     "iopub.status.idle": "2022-02-12T01:22:21.492525Z",
     "shell.execute_reply": "2022-02-12T01:22:21.491078Z",
     "shell.execute_reply.started": "2022-02-12T01:06:23.983614Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of the dataset is :  543420\n",
      "118\n",
      "Data padded!\n",
      "Found 1193514 word vectors.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 118, 200)          108684200 \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 118, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 108,811,129\n",
      "Trainable params: 126,929\n",
      "Non-trainable params: 108,684,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "15000/15000 [==============================] - 56s 4ms/step - loss: 0.3793 - accuracy: 0.8316\n",
      "LSTM Accuracy: 83.16%\n"
     ]
    }
   ],
   "source": [
    "lstm_model.fit_lstm(sets_raw_sent140,epochs=10, batch_size=128,embedding_dim=200,\n",
    "                    embeddings_name=GLOVE_EMB_TWI_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DistilBert import DistilBert\n",
    "distilbert_model=DistilBert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DistilBert on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:20:03.790523Z",
     "iopub.status.busy": "2022-02-12T17:20:03.789659Z",
     "iopub.status.idle": "2022-02-12T17:37:42.382243Z",
     "shell.execute_reply": "2022-02-12T17:37:42.380235Z",
     "shell.execute_reply.started": "2022-02-12T17:20:03.790485Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Tokenizing completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 17:27:50.234867: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-12 17:27:50.236804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-12 17:27:50.237957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-12 17:27:50.239066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-12 17:27:56.031913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-12 17:27:56.033119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-12 17:27:56.034287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-12 17:27:56.035406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14959 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3544000fe647c1a8017260553ba1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/347M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 17:28:11.131690: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Building the model----\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 768)          0           tf_distil_bert_model[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          393728      tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 66,757,121\n",
      "Trainable params: 66,757,121\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 17:28:27.131209: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930/930 [==============================] - 485s 510ms/step - loss: 0.2715 - accuracy: 0.8871 - val_loss: 0.2130 - val_accuracy: 0.9168\n",
      "469/469 [==============================] - 79s 167ms/step - loss: 0.2185 - accuracy: 0.9148\n",
      "Test score: [0.2185227870941162, 0.9147999882698059]\n"
     ]
    }
   ],
   "source": [
    "distilbert_model.fit_distil_bert(sets_raw_imdb,1,256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DistilBert on Sentiment140 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:38:46.017625Z",
     "iopub.status.busy": "2022-02-12T17:38:46.017313Z",
     "iopub.status.idle": "2022-02-12T19:11:43.685497Z",
     "shell.execute_reply": "2022-02-12T19:11:43.684527Z",
     "shell.execute_reply.started": "2022-02-12T17:38:46.017593Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Tokenizing completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Building the model----\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_1 (TFDisti TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (None, 768)          0           tf_distil_bert_model_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          393728      tf.__operators__.getitem_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            513         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 66,757,121\n",
      "Trainable params: 66,757,121\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "29750/29750 [==============================] - 3734s 125ms/step - loss: 0.3487 - accuracy: 0.8469 - val_loss: 0.3236 - val_accuracy: 0.8603\n",
      "15000/15000 [==============================] - 717s 48ms/step - loss: 0.3224 - accuracy: 0.8610\n",
      "Test score: [0.32236185669898987, 0.8609562516212463]\n"
     ]
    }
   ],
   "source": [
    "distilbert_model.fit_distil_bert(sets_raw_sent140,epochs=1,max_len=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "\n",
    "The logistic regression achieves better accuracy on the raw data. While the difference is very small for the IMDB reviews, it's quite significant for tweets from the Sentiment140 dataset.\n",
    "\n",
    "IMDB raw|IMDB preprocessed|Sentiment140 raw|Sentiment140 preprocessed\n",
    "-|-|-|-\n",
    "89.20|89.04|82.07|77.85\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DL Models\n",
    "\n",
    "As it was the case with our baseline model, both LSTM and CNN-LSTM models perform better when applied to the raw data. Our CNN-LSTM model, however, didn't outperform neither the baseline nor the LSTM model. The latter model does give better accuracy compared to the logistic regression, but even for the highest results the improvement over the baseline model is marginal (around 1%).\n",
    "\n",
    "Below you can find an overview of the results for both datasets. While presenting a bunch of results from running the models with slightly different parameters usually doesn't belong to the best practices of ML, we did it in this essay on purpose to illustrate the effect of changing the embeddings, their dimensions, and epochs. \n",
    "\n",
    "We found out that pretrained embeddings had a slight positive effect on the models' performance. This is consistent with our reviewed literature. As expected, LSTM with GloVe embeddings which were trained on Wikipedia & Gigaword texts performed better than LSTM with embeddings trained on tweets. The opposite was true for the Sentiment140 dataset.\n",
    "\n",
    "We can also observe a trend where increased dimensionality of the embeddings leads to slight improvements in the models' accuracy. This is most visible for the pretrained embeddings while the results for embeddings which were trained from scratch are not so straightforward.\n",
    "\n",
    "Finally, increasing the number of epochs for the LSTM model with pretrained embeddings also provides us with small accuracy gains. Interestingly, this is not the case with the embeddings trained from scratch. Here our results for both datasets usually got worse with increased number of epochs.\n",
    "- IMDB dataset\n",
    "\n",
    "Dimension|Embeddings|Batch size|Epochs|LSTM|CNN-LSTM||LSTM|CNN-LSTM\n",
    "-|-|-|-|-|-|-|-|-\n",
    "||||raw |raw ||preprocessed|preprocessed \n",
    "100|GloVe Wiki |128|3|87.39|87.16||88.85|85.75\n",
    "100|GloVe Twitter |128|3|85.94|87.02||83.85|84.62\n",
    "100||128|3|**89.31**|84.77||87.83|87.74\n",
    "100|GloVe Wiki |128|20|**90.18**|88.25||86.94|85.65\n",
    "100|GloVe Twitter|128|20|**89.47**|88.02||86.80|84.55\n",
    "100||128|20|87.07|87.63||86.32|86.30\n",
    "||||||||\n",
    "200|GloVe Wiki |128|3|87.09|88.70||86.47|86.62\n",
    "200|GloVe Twitter |128|3|86.01|86.42||85.95|86.37\n",
    "200||128|3|86.14|88.15||87.00|87.16\n",
    "200|GloVe Wiki |128|20|**89.97**|87.21||86.82|85.89\n",
    "||||||||\n",
    "300|GloVe Wiki |128|3|88.60|88.91||89.03|87.44\n",
    "300||128|3|89.01|88.90||87.10|87.05\n",
    "300|GloVe Wiki|128|20|**90.31**|88.52||87.88|86.28|\n",
    "300||128|20|87.27|88.11||86.47|86.21\n",
    "||||||||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentiment140 dataset\n",
    "\n",
    "Dimension|Embeddings|Batch size|Epochs|LSTM|CNN-LSTM||LSTM|CNN-LSTM\n",
    "-|-|-|-|-|-|-|-|-\n",
    "||||raw data|raw data||preprocessed data|preprocessed data\n",
    "25|GloVe Twitter|2048|3|76.42|72.40||73.82|72.40\n",
    "25|GloVe Twitter|512|3|78.61|75.85||75.14|73.54\n",
    "25|GloVe Twitter|128|3|79.58|76.78||75.53|73.92\n",
    "||||||||\n",
    "50|GloVe Twitter|2048|3|79.34|78.36||75.98|75.00\n",
    "50|GloVe Twitter|128|3|81.58|79.89||76.86|75.87\n",
    "50|GloVe Wiki|128|3|80.05|78.03||75.96|74.09\n",
    "50||128|3|79.14|80.41||77.46|77.26\n",
    "||||||||\n",
    "100|GloVe Twitter|128|3|**82.28**|81.29||76.98|76.55\n",
    "100|GloVe Twitter|128|10|**82.81**|81.95||77.93|77.01\n",
    "100|GloVe Wiki|128|3|81.10|80.27||76.53|75.62\n",
    "100|GloVe Wiki|128|10|82.05|80.81||76.94|75.79\n",
    "100||128|3|80.15|78.75||77.44|76.99\n",
    "||||||||\n",
    "200|GloVe Twitter|128|10|**83.16**|**82.35**||77.97|76.95\n",
    "200|GloVe Wiki|128|10|**82.26**|77.05||77.11|75.87\n",
    "200||128|3|80.40|80.06||77.22|76.66\n",
    "200||128|10|79.22|79.03||76.31|74.60\n",
    "||||||||\n",
    "300|GloVe Wiki|128|10|**82.41**|75.58||77.18|75.61\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer: DistilBert\n",
    "\n",
    "Our DistilBert model does outperform the baseline and LSTM models for both datasets. Consistent with our previous findings, the results are better when the model is trained on and applied to the raw data. In the table below we only present the results from training the model for one epoch, as the accuracy decreased with increasing number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs|IMDB raw|IMDB preprocessed|Sentiment140 raw|Sentiment140 preprocessed\n",
    "-|-|-|-|-\n",
    "1|**91.47**|88.32|**86.10**|78.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best results: Accuracy vs. Time\n",
    "\n",
    "While the accuracy gain of DistilBert is, again, rather small for the IMDB reviews predictions, it is more significant for the Sentiment140 dataset with around +4% vs. baseline. This, however, goes with a significant increase in training time. \n",
    "\n",
    "Data/Time|LogReg|LSTM|CNN-LSTM|DistilBert\n",
    "-|-|-|-|-\n",
    "IMDB (raw)|89.20|90.31|88.90|91.47\n",
    "Time|3.5 min|19 min|11 min|14 min\n",
    "||||\n",
    "Sentiment140 (raw)|82.07|83.16|82.35|86.10\n",
    "Time||15 min|14 min|1h 30 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessed Data vs. Raw Data\n",
    "We were surprised to see that running the different classifiers on the raw data of our datasets actually brought better performance scores than the preprocessed versions. This made us think that our preprocessing might have been ripping off parts of the information of the texts and might have led to wrong predictions. Another thought is that the more complex the sentences and more irony / sarcasm are „hidden“ in the texts, the more DL models are the better approach in comparison to traditional ML models. Unfortunately with scarce time at the end of the semester, we couldn’t look deeper into that topic but would recommend this as a topic for further research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "\n",
    "Consistent with the current state of the art, our LSTM model performed better than the baseline logistic regression model, and DistilBert provides the best results on the sentiment classification task. This, however, requires additional time / computational resources. The appropriatness of application of complex models would depend on the specific use cases. In many real-world business applications, marginal accuracy improvements might not justify increases in computational ressources. This is especially true if classifications are made on a frequent basis. At the same time, the training, of course, doesn't necessarily need to be done each time before the classification. It might be feasible to make an one-time-investment to train the transformer model once, save and reuse it.\n",
    "\n",
    "As for the classroom setting, the choice of the appropriate model would strongly depend on the available computational resources and the overall application setting. If the goal is to explore / improve the state-of-the-art, transformers are surely the best choice. Due to time constraints we weren't able to explore all possible fine-tuning opportunities for our DistilBert model and, nevertheless, did get good results. Therefore, it would be intersting to explore whether further fine-tuning of the model (including the preprocessing of the respective data) can result in better performance and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaye, B., Zhang, D., & Wulamu, A. (2021). A Tweet Sentiment Classification Approach Using a Hybrid Stacked Ensemble Technique. Information, 12(9), 374.\n",
    "\n",
    "Alsayat, A. (2021). Improving Sentiment Analysis for Social Media Applications Using an Ensemble Deep Learning Language Model. Arabian Journal for Science and Engineering, 1-13.\n",
    "\n",
    "Vizcarra, J., Kozaki, K., Ruiz, M. T., & Quintero, R. (2021). Knowledge-based sentiment analysis and visualization on social networks. New Generation Computing, 39(1), 199-229.\n",
    "\n",
    "B. Pang, L. Lee, S. Vaithyanathan. (2002). Thumbs up? Sentiment Classification using\n",
    "Machine Learning Techniques. Proceedings of EMNLP 2002. pp. 79-86.\n",
    "\n",
    "Q. Ain, M. Ali, A. Riaz, A. Noureen, M. Kamran, B. Hayat, A. Rehman. (2017). Sentiment\n",
    "Analysis using Deep Learning techniques. (IJACSA) International Journal of Advanced\n",
    "Computer Science and Applications, Vol. 8, No. 6.\n",
    "\n",
    "B. Pang, L. Lee. (2008). Opinion Mining and Sentiment Analysis. Foundations and Trends in\n",
    "Information Retrieval Vol. 2, Nos. 1–2. DOI: 10.1561/1500000001. pp. 1-135.\n",
    "\n",
    "P. Ekman, W. Friesen, P. Ellsworth. (1982). What emotion categories or dimensions can\n",
    "observers judge from facial behavior?. Emotion in the human face. Cambridge University\n",
    "Press, New York. pp 39-55.\n",
    "\n",
    "M. Kurosu. (2015). Human-Computer Interaction. 17th International Conference, HCI\n",
    "International 2015, Los Angeles. Proceedings, Part II. p. 423. [book]\n",
    "\n",
    "X. Wang, Y. Liu, C. Sun, B. Wang, X. Wang. (2015). Predicting Polarities of Tweets by\n",
    "Composing Word Embeddings with Long Short-Term Memory. Proceedings of the 53rd\n",
    "Annual Meeting of the Association for Computational Linguistics and the 7th International\n",
    "Joint Conference on Natural Language Processing. pp. 1343-1353.\n",
    "\n",
    "X. Wang, C. Zhang, Y. Ji, L. Sun, L. Wu, Z. Bao. (2013). A Depression Detection Model\n",
    "Based on Sentiment Analysis in Micro-blog Social Network. PAKDD 2013: Trends and\n",
    "Applications in Knowledge Discovery and Data Mining. pp. 201-213.\n",
    "\n",
    "R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuglu, P. Kuksa. (2011). Natural\n",
    "Language Processing (Almost) from Scratch. Journal of Machine Learning Research 12. pp.\n",
    "2493-2537.\n",
    "\n",
    "Bacco, L., Cimino, A., Dell’Orletta, F., & Merone, M. (2021). Explainable Sentiment Analysis: A Hierarchical Transformer-Based Extractive Summarization Approach. Electronics, 10(18), 2195.\n",
    "\n",
    "Wu, Z., Ying, C., Dai, X., Huang, S., & Chen, J. (2020, October). Transformer-Based Multi-aspect Modeling for Multi-aspect Multi-sentiment Analysis. In CCF International Conference on Natural Language Processing and Chinese Computing (pp. 546-557). Springer, Cham.\n",
    "\n",
    "Jiang, M., Wu, J., Shi, X., & Zhang, M. (2019). Transformer based memory network for sentiment analysis of web comments. IEEE Access, 7, 179942-179953.\n",
    "\n",
    "Novikova, A., & Stupnikov, S. (2017, July). Sentiment analysis of short texts from social networks using sentiment lexicons and blending of machine learning algorithms. In Proc. CEUR Workshop (pp. 190-201).\n",
    "\n",
    "Singh, J., Singh, G., & Singh, R. (2017). Optimization of sentiment analysis using machine learning classifiers. Human-centric Computing and information Sciences, 7(1), 1-12.\n",
    "\n",
    "Rustam, F., Ashraf, I., Mehmood, A., Ullah, S., & Choi, G. S. (2019). Tweets classification on the base of sentiments for US airline companies. Entropy, 21(11), 1078.\n",
    "\n",
    "Ahuja, R., Chug, A., Kohli, S., Gupta, S., & Ahuja, P. (2019). The impact of features extraction on the sentiment analysis. Procedia Computer Science, 152, 341-348.\n",
    "\n",
    "PURCHASES, C. J. O. I., STOYANOVA, L., & WALLACE, W. (2019). TOPIC MODELLING, SENTIMENT ANALSYS AND CLASSIFICATION OF SHORT-FORM TEXT.\n",
    "\n",
    "da Silva, N. F. F., Coletta, L. F., Hruschka, E. R., & Hruschka Jr, E. R. (2016). Using unsupervised information to improve semi-supervised tweet sentiment classification. Information Sciences, 355, 348-365.\n",
    "\n",
    "Araque, O., Corcuera-Platas, I., Sánchez-Rada, J. F., & Iglesias, C. A. (2017). Enhancing deep learning sentiment analysis with ensemble techniques in social applications. Expert Systems with Applications, 77, 236-246.\n",
    "\n",
    "Yi, S., & Liu, X. (2020). Machine learning based customer sentiment analysis for recommending shoppers, shops based on customers’ review. Complex & Intelligent Systems, 6(3), 621-634.\n",
    "\n",
    "Ouyang, X., Zhou, P., Li, C. H., & Liu, L. (2015, October). Sentiment analysis using convolutional neural network. In 2015 IEEE international conference on computer and information technology; ubiquitous computing and communications; dependable, autonomic and secure computing; pervasive intelligence and computing (pp. 2359-2364). IEEE.\n",
    "\n",
    "Jiang, M., Wu, J., Shi, X., & Zhang, M. (2019). Transformer based memory network for sentiment analysis of web comments. IEEE Access, 7, 179942-179953.\n",
    "\n",
    "Wu, Z., Ying, C., Dai, X., Huang, S., & Chen, J. (2020, October). Transformer-Based Multi-aspect Modeling for Multi-aspect Multi-sentiment Analysis. In CCF International Conference on Natural Language Processing and Chinese Computing (pp. 546-557). Springer, Cham.\n",
    "\n",
    "Elfaik, H. (2021). Deep Bidirectional LSTM Network Learning-Based Sentiment Analysis for Arabic Text. Journal of Intelligent Systems, 30(1), 395-412.\n",
    "\n",
    "Alexandridis, G., Varlamis, I., Korovesis, K., Caridakis, G., & Tsantilas, P. (2021). A Survey on Sentiment Analysis and Opinion Mining in Greek Social Media. Information, 12(8), 331.\n",
    "\n",
    "Flender, M., & Gips, C. (2017, September). Sentiment Analysis of a German Twitter-Corpus. In LWDA (p. 25).\n",
    "\n",
    "Tellez, E. S., Miranda-Jiménez, S., Graff, M., Moctezuma, D., Siordia, O. S., & Villaseñor, E. A. (2017). A case study of Spanish text transformations for twitter sentiment analysis. Expert Systems with Applications, 81, 457-471.\n",
    "\n",
    "Carosia, A. E. O., Coelho, G. P., & Silva, A. E. A. (2020). Analyzing the Brazilian financial market through Portuguese sentiment analysis in social media. Applied Artificial Intelligence, 34(1), 1-19.\n",
    "\n",
    "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).\n",
    "\n",
    "Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training.\n",
    "\n",
    "Jain, P. K., Saravanan, V., & Pamula, R. (2021). A hybrid CNN-LSTM: A deep learning approach for consumer sentiment analysis using qualitative user-generated contents. Transactions on Asian and Low-Resource Language Information Processing, 20(5), 1-15.\n",
    "\n",
    "Ligthart, A., Catal, C., & Tekinerdogan, B. (2021). Systematic reviews in sentiment analysis: a tertiary study. Artificial intelligence review, 54(7), 4997-5053. \n",
    "\n",
    "Pennington, J., Socher, R., & Manning, C. D. (2014, October). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).\n",
    "\n",
    "Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
