{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fa68ae-687b-4082-bb3d-6ab26b0c2d5b",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "### Benchmarking State-of-the-Art Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72bae2c",
   "metadata": {},
   "source": [
    "Oleksandra Kovalenko (578447)   \n",
    "Cosima Heymann (569413)  \n",
    "Sascha Geyer (546266)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a67f2",
   "metadata": {},
   "source": [
    "![sentiment](https://camo.githubusercontent.com/899f79e8a2d62fd642eba0791ff66d13d38e427901bfc3cd89c6f613311e1789/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f70726f78792f312a5f4a57314a614d704b5f6656476c64387064315f4a512e676966 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a171b2ae-0f7f-4590-a943-300e1aabbbd6",
   "metadata": {},
   "source": [
    "### Introduction: What is Sentiment Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6f362",
   "metadata": {},
   "source": [
    "The growth of user-generated content in web sites and social networks, just to mention a few: Yelp, Twitter, Amazon, Tripadvisor, Rottentomatoes and IMDB has led to an increasing power for expressing opinions. In recent years, the automatic extraction of opinions from a text has become an area of growing interest in Natural Language Processing (NLP). Online opinions have turned into a valuable asset since the fast spreading nature of online content. In order to analyze the massive amount of data, many NLP tasks are being used. In particular, Sentiment Analysis, also known as Opinion Mining (from now on: SA), became an increasingly growing task, whose goal it is to classify opinions and sentiments expressed in user-generated text. SA is on the rise due to the increased requirement of analyzing and structuring hidden information, which comes from user-generated content in the form of unstructured data (Ain, Ali, Riaz, Noureen, Kamran, Hayat & Rehman, 2017). It allows to detect the emotion and sentiment that an author of a text felt towards a described subject or entity. It is interesting in many fields and branches and helps solving various tasks, e.g.:\n",
    "\n",
    "- companies are able to measure the feedback about a product or service,\n",
    "- sociologists can look at people’s reaction about certain public events,\n",
    "- psychologists can study the general mind state of communities with regard to various issues, i.e. a depression detection model that is based on SA in micro-blog social networks (Wang, Zhang, Ji, Sun, Wu & Bao, 2013),\n",
    "- governments and political parties are able to correct their actions according to social approval or disapproval,\n",
    "- etc.\n",
    "\n",
    "The challenge is that sentiments are not always expressed explicitly and meanings can be hidden in the context. In these cases, additional word and language knowledge is necessary. Moreover, opinions may involve sarcasm and negations, which can be interpreted differently in various domains and contexts. Sentiment classification is rather easy for humans (Pang, Lee & Vaithyanathan, 2002), but manual review and analysis of texts is very time consuming and expensive. Due to this fact, automatic sentiment classifiers are selected instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff969c0",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis: Definition, Application & Classification \n",
    "\n",
    "Sentiment Analysis is an active research area in NLP that refers to the use of text analysis, statistical learning and often Machine Learning to extract subjective information in source materials such as user-generated texts from social networks, blogs, forums and product or service reviews.\n",
    "Selecting the basic emotions is a difficult task for a computer because of the variety of human emotions. Most of the authors in the NLP community agree on the classification proposed by Ekman, Friesen and Ellsworth (1982) that six basic emotions exist: anger, disgust, fear, joy, sadness and surprise. As such a division requires a complex processing and analysis of the input data, the majority of researchers and authors accept a simpler representation of sentiments according to their polarity (Pang & Lee, 2008). Kurosu (2015) defines sentiment polarity as follows: “The polarity of a sentiment is the point on the evaluation scale that corresponds to our positive or negative evaluation of the meaning of this sentiment.”. Sentiment polarity allows researchers to use a binary or ternary measurement, either positive, negative or neutral and therefore, simplifies the representation and management of the sentiment information. The granularity of SA can be either coarse-grained or fine-grained. Coarse-grained means usually a binary classification (positive, negative). On ther other hand, fine-grained uses for example five possible levels of granularity (high positive, low positive, neutral, low negative, high negative). Liu et al. (2015) presented three levels of SA: document level, sentence level and entity / aspect level. \n",
    "\n",
    "While document level studies the polarity of the whole text with respect to a single entity (e.g. a product), sentence level studies the polarity of single sentences, analyzing clauses and phrases for its sentiment. Contrary, entity / aspect level analyzes what people especially liked or disliked. An entity-aspect might be a single token and its polarity might be different from the overall polarity of the text (Liu et al., 2015).\n",
    "\n",
    "#####  Application\n",
    "To mention a few application areas:\n",
    "\n",
    "- Social media monitoring,\n",
    "- Customer support / feedback,\n",
    "- Brand monitoring and reputation management,\n",
    "- Voice of customer (VoC),\n",
    "- Voice of employee,\n",
    "- Product analysis,\n",
    "- Market research and competitive research.\n",
    "\n",
    "##### Classification\n",
    "\n",
    "All methods used to solve sentiment classification fall into three main categories: lexicon-based, machine learning-based and hybrid approaches.\n",
    "\n",
    "In lexicon-based approaches, also known as knowledge-based methods, sentiment is seen as a function of keywords and is based on their count. The main task is the construction of sentiment word lexicons with the indicated class labels positive or negative. In some cases also with their intensiveness, which becomes important for a fine-grained classification.\n",
    "\n",
    "An alternative to the knowledge-based method is Machine Learning, which is gaining more and more interest of researchers due to its adaptability and higher accuracy. Traditional Machine Learning methods were the dominant approach in SA (Pang et al., 2002) with the three main algorithms: Naïve Bayes (NB), Support Vector Machines (SVM) and Maximum Entropy (MaxEnt, in Statistics called: Logisitic Regression). Part of Machine Learning models are Deep Learning models (DL) and Transformer models.\n",
    "\n",
    "The hybrid approach, also known as combined analysis or ensemble models, combines both knowledge-based and Machine Learning-based methods and thus, can lead to a superior performance. Researchers were attracted to explore the possibility of a hybrid approach that collectively could exhibit the accuracy of a Machine Learning approach and the speed of a lexical approach.\n",
    "\n",
    "Although traditional Machine Learning algorithms like Support Vector Machines have shown good performance in various NLP tasks for the past decades, they have a few shortcomings, where DL has the potential to overcome these limitations to a large extent and has already shown excellent performance in NLP tasks, including SA (Collobert, Weston, Bottou, Karlen, Kavukcuglu & Kuksa, 2011). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5159d44-5fe5-440b-b8b6-6d5e6333fb21",
   "metadata": {},
   "source": [
    "### Research Overview\n",
    "\n",
    "- MISSING: what is the state-of-the-art research towards 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6b812",
   "metadata": {},
   "source": [
    "The following section describes related works that exploits ML and DL approaches to solve SA tasks on different data sets and from different perspectives in the past 5 years. This review is conducted on the basis of numerous latest studies and researches in the field of SA. The first table presents several methods for English texts, whereas the second literature table mentions a few papers for languages like Greek, German or French. This field of research (SA for different languages) is for sure a topic for future studies.\n",
    "\n",
    "There are also several papers that exploit the methods of lexicon-based models but we have focused on ML-based techniques and ensemble methods. If you want to get an overview of these traditional models, you will find many papers in the world wide web, i.e. the paper from Vizcarra et al. (2021). \n",
    "\n",
    "Hybrid: \n",
    "Gaye, B., Zhang, D., Wulamu, A. (2021)\n",
    "Novikova, A., Stupnikov, S. (2017)\n",
    "Alsayat, A. (2021)\n",
    "Araque, O., Corcuera-Platas, I., Sánchez-Rada, J. F., & Iglesias, C. A. (2017)\n",
    "\n",
    "Transformer:\n",
    "Bacco, L., Cimino, A., Dell’Orletta, F., & Merone, M. (2021)\n",
    "Jiang, M., Wu, J., Shi, X., & Zhang, M. (2019)\n",
    "Wu, Z., Ying, C., Dai, X., Huang, S., & Chen, J. (2020)\n",
    "\n",
    "Machine Learning / Deep Learning:\n",
    "Singh, J., Singh, G., & Singh, R. (2017)\n",
    "Rustam, F., Ashraf, I., Mehmood, A., Ullah, S., & Choi, G. S. (2019)\n",
    "Ahuja, R., Chug, A., Kohli, S., Gupta, S., & Ahuja, P. (2019)\n",
    "PURCHASES, C. J. O. I., STOYANOVA, L., & WALLACE, W. (2019)\n",
    "da Silva, N. F. F., Coletta, L. F., Hruschka, E. R., & Hruschka Jr, E. R. (2016)\n",
    "Yi, S., & Liu, X. (2020)\n",
    "Ouyang, X., Zhou, P., Li, C. H., & Liu, L. (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e127d28",
   "metadata": {},
   "source": [
    "English:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Paper Name</strong>\n",
    "   </td>\n",
    "   <td><strong>Year of Publication</strong>\n",
    "   </td>\n",
    "   <td><strong>Dataset(s)</strong>\n",
    "   </td>\n",
    "   <td><strong>Classification</strong>\n",
    "   </td>\n",
    "   <td><strong>Algorithms</strong>\n",
    "   </td>\n",
    "   <td><strong>Performance Evaluation Criteria</strong>\n",
    "   </td>\n",
    "   <td><strong>Source</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Explainable Sentiment Analysis: A Hierarchical Transformer-Based Extractive Summarization Approach\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>IMDB\n",
    "   </td>\n",
    "   <td>Transformer \n",
    "   </td>\n",
    "   <td>Explainable Hierarchical Transformer (ExHiT),  Sentence Classification Combiner Model (SCC)\n",
    "   </td>\n",
    "   <td>accuracy\n",
    "   </td>\n",
    "   <td><a href=\"https://www.mdpi.com/2079-9292/10/18/2195/pdf\">https://www.mdpi.com/2079-9292/10/18/2195/pdf</a>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>A Tweet Sentiment Classification Approach Using a Hybrid Stacked Ensemble Technique\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>Sentiment140\n",
    "   </td>\n",
    "   <td>Hybrid \n",
    "   </td>\n",
    "   <td>stacked ensemble of three long short-term memory (LSTM) as base classifiers and logistic regression (LR) as a meta classifier\n",
    "   </td>\n",
    "   <td>accuracy, F1 \n",
    "   </td>\n",
    "   <td><a href=\"https://www.mdpi.com/2078-2489/12/9/374\">https://www.mdpi.com/2078-2489/12/9/374</a>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Optimization of sentiment analysis using machine learning classifiers\n",
    "   </td>\n",
    "   <td>2017\n",
    "   </td>\n",
    "   <td>3 manually compiled datasets; two of them are captured from Amazon and one dataset is assembled from IMDB movie reviews\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>Naïve Bayes, J48, BFTree and OneR\n",
    "   </td>\n",
    "   <td>accuracy, F-measure, correctly classified instances\n",
    "   </td>\n",
    "   <td><a href=\"https://doi.org/10.1186/S13673-017-0116-3\">https://doi.org/10.1186/S13673-017-0116-3</a>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Sentiment Analysis of Short Texts from Social Networks Using Sentiment Lexicons and Blending of Machine Learning Algorithms\n",
    "   </td>\n",
    "   <td>2017\n",
    "   </td>\n",
    "   <td>VKontakte social network posts\n",
    "   </td>\n",
    "   <td>Hybrid\n",
    "   </td>\n",
    "   <td>Logistic Regression, Random Forest Classifier, SVM, Gradient Boosting Classifier, KNeighbors Classifier, Multinomial Naive Bayes\n",
    "   </td>\n",
    "   <td>F1 \n",
    "   </td>\n",
    "   <td>http://ceur-ws.org/Vol-2268/paper21.pdf\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Tweets Classification on the Base of Sentiments for US Airline Companies\n",
    "   </td>\n",
    "   <td>2019\n",
    "   </td>\n",
    "   <td>Twitter US Airline Sentiment\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>Voting Classifier based on logistic regression (LR) and stochastic gradient descent classifier (SGDC) <strong>vs</strong> a variety of machine learning classifiers\n",
    "   </td>\n",
    "   <td>accuracy, F1\n",
    "   </td>\n",
    "   <td><a href=\"https://doi.org/10.3390/e21111078\">https://doi.org/10.3390/e21111078</a>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>The Impact of Features Extraction on the Sentiment Analysis\n",
    "   </td>\n",
    "   <td>2019\n",
    "   </td>\n",
    "   <td>Sentiment Strength Twitter Dataset\t\t\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>TFIDF vs N-gram on 6 ML algos (LR, SVM, Decision Tree, Random Forest, KNN, Naive Bayes)\n",
    "   </td>\n",
    "   <td>accuracy, F1 \n",
    "   </td>\n",
    "   <td>https://www.sciencedirect.com/science/article/pii/S1877050919306593\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>TOPIC MODELLING, SENTIMENT ANALSYS AND CLASSIFICATION OF SHORT-FORM TEXT\n",
    "   </td>\n",
    "   <td>2019\n",
    "   </td>\n",
    "   <td>data was obtained through\n",
    "Twitter and Facebook’s public APIs with Netlytic\n",
    "   </td>\n",
    "   <td>Lexicon-based, Machine Learning, Deep Learning\n",
    "   </td>\n",
    "   <td>LDA (Latent Dirichlet Allocation), \n",
    "LSA (Latent Semantic Allocation) vs LR, SVM and Naive Bayes\n",
    "   </td>\n",
    "   <td>technical performance (perplexity score and topic coherence score), ease of application, as well as proximity to human agent performance on the same problem\n",
    "   </td>\n",
    "   <td>https://local.cis.strath.ac.uk/wp/extras/msctheses/papers/strath_cis_publication_2733.pdf\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Using unsupervised information to improve semi-supervised tweet sentiment classification\n",
    "   </td>\n",
    "   <td>2016\n",
    "   </td>\n",
    "   <td>6 datasets: SemEval 2013, LiveJournal, SMS2013, Twitter2013, Twitter2014, Twitter Sarcasm 2014 \n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>semi-supervised C3E algorithmvs SVM\n",
    "   </td>\n",
    "   <td>F-Scores\n",
    "   </td>\n",
    "   <td>https://www.researchgate.net/publication/295244270_Using_unsupervised_information_to_improve_semi-supervised_tweet_sentiment_classification\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Improving Sentiment Analysis for Social Media Applications Using an Ensemble Deep Learning Language Model\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>3 datasets: own Twitter coronavirus hashtag dataset as well as public review datasets from Amazon and Yelp\n",
    "   </td>\n",
    "   <td>Hybrid\n",
    "   </td>\n",
    "   <td>customized deep learning model with an advanced word embedding technique and create a long short-term memory (LSTM)\n",
    "   </td>\n",
    "   <td>accuracy\n",
    "   </td>\n",
    "   <td>https://pubmed.ncbi.nlm.nih.gov/34660170/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Enhancing Deep Learning Sentiment Analysis with Ensemble Techniques in Social Applications\n",
    "   </td>\n",
    "   <td>2017\n",
    "   </td>\n",
    "   <td>7 datasets on movie reviews and microblogging \n",
    "   </td>\n",
    "   <td>Deep Learning, Hybrid\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>F1 \n",
    "   </td>\n",
    "   <td>https://www.researchgate.net/publication/313332224_Enhancing_Deep_Learning_Sentiment_Analysis_with_Ensemble_Techniques_in_Social_Applications\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Machine learning based customer sentiment analysis for recommending shoppers, shops based on customers’ review\n",
    "   </td>\n",
    "   <td>2020\n",
    "   </td>\n",
    "   <td>product data with customer reviews is collected from benchmark Unified computing system (UCS)\n",
    "   </td>\n",
    "   <td>Machine Learning \n",
    "   </td>\n",
    "   <td>Hybrid Recommendation System\n",
    "   </td>\n",
    "   <td>MAPE\n",
    "   </td>\n",
    "   <td>https://link.springer.com/article/10.1007/s40747-020-00155-2\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Sentiment Analysis Using Convolutional Neural Network\n",
    "   </td>\n",
    "   <td>2015\n",
    "   </td>\n",
    "   <td>IMDB\n",
    "   </td>\n",
    "   <td>Deep Learning\n",
    "   </td>\n",
    "   <td>RNN, LSTM, CNN,\n",
    "   </td>\n",
    "   <td>accuracy\n",
    "   </td>\n",
    "   <td>https://ieeexplore.ieee.org/abstract/document/7363395\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Transformer Based Memory Network for Sentiment Analysis of Web Comments\n",
    "   </td>\n",
    "   <td>2019\n",
    "   </td>\n",
    "   <td>2 datasets: Weibo, Semeval \n",
    "   </td>\n",
    "   <td>Transformer\n",
    "   </td>\n",
    "   <td>Transformer based memory network (TF-MN)\n",
    "   </td>\n",
    "   <td>accuracy, F1 \n",
    "   </td>\n",
    "   <td>https://www.researchgate.net/publication/337697651_Transformer_Based_Memory_Network_for_Sentiment_Analysis_of_Web_Comments\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Transformer-based Multi-Aspect Modeling for Multi-Aspect Multi-Sentiment Analysis\n",
    "   </td>\n",
    "   <td>2020\n",
    "   </td>\n",
    "   <td>MultiAspect Multi-Sentiment (MAMS) dataset\n",
    "   </td>\n",
    "   <td>Transformer\n",
    "   </td>\n",
    "   <td>RoBERTa-Transformer-based\n",
    "Multi-aspect Modeling method (TMM)\n",
    "   </td>\n",
    "   <td>accuracy, F1\n",
    "   </td>\n",
    "   <td>https://arxiv.org/abs/2011.00476\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<strong>Table 1. Literature Review for English Datasets</strong>\n",
    "\n",
    "SA in languages other than English became a more popular research area in the last 1-2 years. Usually, they would use Machine Learning / Deep Learning based methods. A few papers that we looked at are:\n",
    "\n",
    "Elfaik, H. (2021)\n",
    "\n",
    "Alexandridis, G., Varlamis, I., Korovesis, K., Caridakis, G., & Tsantilas, P. (2021)\n",
    "\n",
    "Flender, M., & Gips, C. (2017)\n",
    "\n",
    "Tellez, E. S., Miranda-Jiménez, S., Graff, M., Moctezuma, D., Siordia, O. S., & Villaseñor, E. A. (2017)\n",
    "\n",
    "Carosia, A. E. O., Coelho, G. P., & Silva, A. E. A. (2020)\n",
    "\n",
    "Rhouati, A., Berrich, J., Belkasmi, M. G., & Bouchentouf, T. (2018)\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Language</strong>\n",
    "   </td>\n",
    "   <td><strong>Paper Name</strong>\n",
    "   </td>\n",
    "   <td><strong>Year of Publication</strong>\n",
    "   </td>\n",
    "   <td><strong>Dataset(s)</strong>\n",
    "   </td>\n",
    "   <td><strong>Classification</strong>\n",
    "   </td>\n",
    "   <td><strong>Algorithms</strong>\n",
    "   </td>\n",
    "   <td><strong>Performance Evaluation Criteria</strong>\n",
    "   </td>\n",
    "   <td><strong>Source</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Arabic\n",
    "   </td>\n",
    "   <td>Deep Bidirectional LSTM Network Learning-Based Sentiment Analysis for Arabic Text\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>6 benchmark sentiment analysis datasets\n",
    "   </td>\n",
    "   <td>Deep Learning\n",
    "   </td>\n",
    "   <td>Bidirectional LSTM Network (BiLSTM)\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>https://www.degruyter.com/document/doi/10.1515/jisys-2020-0021/html\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Greek\n",
    "   </td>\n",
    "   <td>A Survey on Sentiment Analysis and Opinion Mining in Greek Social Media\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>self-collected and annotated Greek Social Media Texts \n",
    "   </td>\n",
    "   <td>Deep Learning\n",
    "   </td>\n",
    "   <td>PaloBert, GreekBERT\n",
    "   </td>\n",
    "   <td>accuracy, F1\n",
    "   </td>\n",
    "   <td>https://doi.org/10.3390/info12080331\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>German\n",
    "   </td>\n",
    "   <td>Sentiment analysis of a German Twitter-Corpus\n",
    "   </td>\n",
    "   <td>2017\n",
    "   </td>\n",
    "   <td>German tweets from a bigger dataset\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>Multinomial NB,  LinearSVC, Decision Tree Classifier, Maxent Classifier\n",
    "   </td>\n",
    "   <td>accuracy, F1\n",
    "   </td>\n",
    "   <td>http://ceur-ws.org/Vol-1917/paper06.pdf\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Spanish\n",
    "   </td>\n",
    "   <td>A case study of Spanish text transformations for twitter sentiment analysis\n",
    "   </td>\n",
    "   <td>2021\n",
    "   </td>\n",
    "   <td>2 Spanish datasets\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>SVM\n",
    "   </td>\n",
    "   <td>accuracy, computing time\n",
    "   </td>\n",
    "   <td>https://www.sciencedirect.com/science/article/abs/pii/S0957417417302312?via%3Dihub\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>(Brazilian) Portuguese\n",
    "   </td>\n",
    "   <td>Analyzing the Brazilian Financial Market Through Portuguese Sentiment Analysis in Social Media\n",
    "   </td>\n",
    "   <td>2018\n",
    "   </td>\n",
    "   <td>self annotated Twitter dataset on financial market\n",
    "   </td>\n",
    "   <td>Machine Learning\n",
    "   </td>\n",
    "   <td>Naive Bayes, Support Vector Machines, Maximum Entropy and Multilayer Perceptron\n",
    "   </td>\n",
    "   <td>accuracy\n",
    "   </td>\n",
    "   <td>https://www.researchgate.net/profile/Arthur-Carosia/publication/336933355_Analyzing_the_Brazilian_Financial_Market_through_Portuguese_Sentiment_Analysis_in_Social_Media/links/5e67edc24585153fb3d5b305/Analyzing-the-Brazilian-Financial-Market-through-Portuguese-Sentiment-Analysis-in-Social-Media.pdf\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>French\n",
    "   </td>\n",
    "   <td>Sentiment Analysis of French Tweets based on Subjective Lexicon Approach: Evaluation of the use of OpenNLP and CoreNLP Tools\n",
    "   </td>\n",
    "   <td>2018\n",
    "   </td>\n",
    "   <td>French tweets using \"Public Opinion Knowledge (POK)\" platform\n",
    "   </td>\n",
    "   <td>Lexicon based in comparison to Machine Learning\n",
    "   </td>\n",
    "   <td>OpenNLP, CoreNLP, dependency analysis implemented by CoreNLP\n",
    "   </td>\n",
    "   <td>F-Scores\n",
    "   </td>\n",
    "   <td>https://www.researchgate.net/publication/326514882_Sentiment_Analysis_of_French_Tweets_based_on_Subjective_Lexicon_Approach_Evaluation_of_the_use_of_OpenNLP_and_CoreNLP_Tools\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<strong>Table 2. Literature Review for other Languages</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a0201-fbb9-44cd-ac9a-67eb823d6835",
   "metadata": {},
   "source": [
    "### Classifier Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509fae2",
   "metadata": {},
   "source": [
    "Below is an overview of community datasets that are publicly available. We have outlined them with the following features: \n",
    "- name,\n",
    "- platform,\n",
    "- domain,\n",
    "- size, \n",
    "- evaluation, \n",
    "- language,\n",
    "- source.\n",
    "\n",
    "This list is by far not extensive and lacks datasets in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766debea",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Name</strong>\n",
    "   </td>\n",
    "   <td><strong>Platform</strong>\n",
    "   </td>\n",
    "   <td><strong>Domain</strong>\n",
    "   </td>\n",
    "   <td><strong>Size</strong>\n",
    "   </td>\n",
    "   <td><strong>Evaluation (binary or more)</strong>\n",
    "   </td>\n",
    "   <td><strong>Language</strong>\n",
    "   </td>\n",
    "   <td><strong>Source</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Twitter US Airline Sentiment\n",
    "   </td>\n",
    "   <td>Twitter \n",
    "   </td>\n",
    "   <td>US Airline user experiences\n",
    "   </td>\n",
    "   <td>3.42 MB\n",
    "   </td>\n",
    "   <td>ternary = positive, negative, neutral\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Sentiment140\n",
    "   </td>\n",
    "   <td>Twitter\n",
    "   </td>\n",
    "   <td>user responses to different products, brands, or topics\n",
    "   </td>\n",
    "   <td>228 MB Training (1.600.000) \n",
    "   </td>\n",
    "   <td>0 = negative, \n",
    "2 = neutral, 4 = positive\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>http://help.sentiment140.com/for-students\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Stanford Sentiment Treebank\n",
    "   </td>\n",
    "   <td>Rotten Tomatoes\n",
    "   </td>\n",
    "   <td>movie reviews\n",
    "   </td>\n",
    "   <td>10.000\n",
    "   </td>\n",
    "   <td>1-25 (25: most positive)\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://nlp.stanford.edu/sentiment/code.html\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Large IMDB Movie Reviews\n",
    "   </td>\n",
    "   <td>IMDB\n",
    "   </td>\n",
    "   <td>movie reviews\n",
    "   </td>\n",
    "   <td>25.000 training, 25.000 test\n",
    "   </td>\n",
    "   <td>binary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Polarity v2.0\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>movie reviews\n",
    "   </td>\n",
    "   <td>3MB (1000 positive and 1000 negative processed reviews)\n",
    "   </td>\n",
    "   <td>binary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Paper Reviews\n",
    "   </td>\n",
    "   <td>conference of computing\n",
    "   </td>\n",
    "   <td>user’s opinion about a paper\n",
    "   </td>\n",
    "   <td>405\n",
    "   </td>\n",
    "   <td>-2: very negative,\n",
    "-1: negative,\n",
    "0: neutral,\n",
    "1: positive,\n",
    "2: very positive\n",
    "   </td>\n",
    "   <td>English, Spanish\n",
    "   </td>\n",
    "   <td>https://archive.ics.uci.edu/ml/datasets/Paper+Reviews\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Multi-Domain Sentiment Dataset\n",
    "   </td>\n",
    "   <td>Amazon\n",
    "   </td>\n",
    "   <td>reviews of amazon products\n",
    "   </td>\n",
    "   <td>unprocessed: 1.9 GB, processed: 19 MB\n",
    "   </td>\n",
    "   <td>reviews contain ratings from 1 to 5 stars (can be converted to binary)\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://www.cs.jhu.edu/~mdredze/datasets/sentiment/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Opin-Rank Review Dataset\n",
    "   </td>\n",
    "   <td>Tripadvisor, Edmunds\n",
    "   </td>\n",
    "   <td>hotel, car reviews\n",
    "   </td>\n",
    "   <td>300.000\n",
    "   </td>\n",
    "   <td>ratings that can be turned into binary?\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://archive.ics.uci.edu/ml/datasets/opinrank+review+dataset\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Sentiment Lexicons For 81 Languages\n",
    "   </td>\n",
    "   <td>-\n",
    "   </td>\n",
    "   <td>-\n",
    "   </td>\n",
    "   <td>2 text files per language\n",
    "   </td>\n",
    "   <td>binary\n",
    "   </td>\n",
    "   <td>81 languages: Afrikaans to Yiddisch\n",
    "   </td>\n",
    "   <td>https://sites.google.com/site/datascienceslab/projects/multilingualsentiment\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Lexicoder\n",
    "   </td>\n",
    "   <td>-\n",
    "   </td>\n",
    "   <td>-\n",
    "   </td>\n",
    "   <td>2,858 negative sentiment words and 1,709 positive sentiment words\n",
    "   </td>\n",
    "   <td>binary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>http://www.snsoroka.com/data-lexicoder/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>DynaSent\n",
    "   </td>\n",
    "   <td>Dynabench\n",
    "   </td>\n",
    "   <td>naturally occurring sentences with sentences created using the open-source Dynabench Platform\n",
    "   </td>\n",
    "   <td>121,634 sentences\n",
    "   </td>\n",
    "   <td>ternary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://github.com/cgpotts/dynasent\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Amazon Fine Foods\n",
    "   </td>\n",
    "   <td>Amazon\n",
    "   </td>\n",
    "   <td>product reviews\n",
    "   </td>\n",
    "   <td>5.000.000 reviews\n",
    "   </td>\n",
    "   <td>ratings that can be turned into binary?\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://snap.stanford.edu/data/web-FineFoods.html\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Germeval2017\n",
    "   </td>\n",
    "   <td>Social Media \n",
    "   </td>\n",
    "   <td>messages\n",
    "   </td>\n",
    "   <td>22,000 messages from various social media and web sources\n",
    "   </td>\n",
    "   <td>ternary\n",
    "   </td>\n",
    "   <td>German\n",
    "   </td>\n",
    "   <td>https://sites.google.com/view/germeval2017-absa/data\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Yelp_polarity_reviews\n",
    "   </td>\n",
    "   <td>Yelp\n",
    "   </td>\n",
    "   <td>business reviews\n",
    "   </td>\n",
    "   <td>600,000 reviews for training, 38,000 for testing\n",
    "   </td>\n",
    "   <td>binary (1 - bad, 2 - good)\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://www.kaggle.com/irustandi/yelp-review-polarity\">https://www.kaggle.com/irustandi/yelp-review-polarity\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Financial PhraseBank\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>financial news (rated as pos/neg/neutral) for investor\n",
    "   </td>\n",
    "   <td>4840 \n",
    "<p>\n",
    "4 configurations available (size depends on the level of agreement of annotators)\n",
    "   </td>\n",
    "   <td>ternary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://github.com/huggingface/datasets/tree/master/datasets/financial_phrasebank\">https://github.com/huggingface/datasets/tree/master/datasets/financial_phrasebank\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>The SigmaLaw- Aspect-Based-SA dataset\n",
    "   </td>\n",
    "   <td>Court cases\n",
    "   </td>\n",
    "   <td>Legal opinion texts\n",
    "   </td>\n",
    "   <td>2,000 sentences\n",
    "   </td>\n",
    "   <td>ternary\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://osf.io/efrqt/\">https://osf.io/efrqt/\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Skytrax Users Review Dataset \n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>English\n",
    "   </td>\n",
    "   <td>https://github.com/quankiquanki/skytrax-reviews-dataset\">https://github.com/quankiquanki/skytrax-reviews-dataset\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>\n",
    "   </td>\n",
    "   <td>https://mpqa.cs.pitt.edu/corpora/mpqa_corpus/\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<strong>Table 3. Overview of community Datasets</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff06ea",
   "metadata": {},
   "source": [
    "### NLP Preprocessing Pipeline\n",
    "Before diving deep into predictive modeling we need to preprocess our textual data. For this task we setup a Preprocessor Class which runs a NLP Pipeline to take care of all necessary preprocessing tasks such as lemmatization, stopword removal and text cleaning with regular expressions. In addition, the Preprocessor also factorizes the label to obtain a binary encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b426ba9-649a-4388-b64e-5c8096246315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review</td>\n",
       "      <td>sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50001 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0                                                 review  sentiment\n",
       "1      One of the other reviewers has mentioned that ...   positive\n",
       "2      A wonderful little production. <br /><br />The...   positive\n",
       "3      I thought this was a wonderful way to spend ti...   positive\n",
       "4      Basically there's a family where a little boy ...   negative\n",
       "...                                                  ...        ...\n",
       "49996  I thought this movie did a down right good job...   positive\n",
       "49997  Bad plot, bad dialogue, bad acting, idiotic di...   negative\n",
       "49998  I am a Catholic taught in parochial elementary...   negative\n",
       "49999  I'm going to have to disagree with the previou...   negative\n",
       "50000  No one expects the Star Trek movies to be high...   negative\n",
       "\n",
       "[50001 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4898961a-e42f-4f90-961b-3b7cde5f0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Preprocessor import Preprocessor\n",
    "from LogisticRegression import LogisticRegression\n",
    "\n",
    "imdb_df = pd.read_csv(\"./data/IMDB.csv\", names=[\"text\", \"sentiment\"])\n",
    "sentiment140_df = pd.read_csv(\"./data/Sentiment140.csv\", header=None, index_col=False, encoding='latin-1', usecols=[0,5], names=[\"sentiment\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb8a6df4-261f-40c3-b376-ad809da0e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sentiment140_df = sentiment140_df.tail(5000)\n",
    "small_sentiment140_df = small_sentiment140_df.append(sentiment140_df.head(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d2edc63-1364-462b-9269-d7a137f941a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"imdb\": {\n",
    "         \"preprocessor\": {\n",
    "            \"name\": \"imdb\",\n",
    "            \"df\": imdb_df,\n",
    "            \"cache\": True\n",
    "        },\n",
    "        \"logistic_regression\": {\n",
    "            'ngram_range': (1,3),\n",
    "            'random_state': 42,\n",
    "            'max_iter': 100,\n",
    "        }   \n",
    "    },\n",
    "    \"sentiment140\": {\n",
    "       \"preprocessor\": {\n",
    "            \"name\": \"sentiment140\",\n",
    "            \"df\": small_sentiment140_df,\n",
    "            \"cache\": True\n",
    "        },\n",
    "        \"logistic_regression\": {\n",
    "            'ngram_range': (1,3),\n",
    "            'random_state': 42,\n",
    "            'max_iter': 100,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def run(config):\n",
    "    preprocessor = Preprocessor(**config['preprocessor'])\n",
    "    preprocessed_df = preprocessor.run()\n",
    "    lr_model = LogisticRegression(df=preprocessed_df, **config['logistic_regression'])\n",
    "    lr_model.fit(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94872a03-1efe-4a93-a121-b5b46b208d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read imdb_preprocessed.parquet.gzip from cache...\n",
      "Successfully read imdb_preprocessed.parquet.gzip into memory.\n",
      "Logistic Regression validation accuracy: 0.8786\n"
     ]
    }
   ],
   "source": [
    "run(configs[\"imdb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07b01727-25a3-4a6d-b1f2-50eb5f0b52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read sentiment140_preprocessed.parquet.gzip from cache...\n",
      "Successfully read sentiment140_preprocessed.parquet.gzip into memory.\n",
      "Logistic Regression validation accuracy: 0.7256666666666667\n"
     ]
    }
   ],
   "source": [
    "run(configs[\"sentiment140\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d034696",
   "metadata": {},
   "source": [
    "#### Dataset: Movie reviews \n",
    "\n",
    "We will compare three classifier on two datasets that contain movie reviews: \n",
    "- IMDB dataset: 50.000 movie reviews \n",
    "- Stanford Sentiment Treebank: 10.000 reviews \n",
    "\n",
    "##### Deep Learning\n",
    "The DL algorithm of this experiment is an LSTM model, which was built with Keras. Keras is an easy usable, high-level neural network API, which is capable to run on top of either TensorFlow or Theano (Keras Documentation, n.d.). The DL algorithm was built with an LSTM architecture using a Sequential model, which consists of five layers: an embedding layer, two dropout layers, an LSTM layer and an output / dense layer. The Sequential model is a linear stack of layers. \n",
    "\n",
    "##### Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb730f6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Baseline algorithm\n",
    "This experiment used the popular and simple ML algorithm Logistic Regression as a baseline algorithm. Logistic Regression is a statistical technique capable of predicting a binary outcome. Using the built-in functions of scikit-learn, the Logistic Regression was very easy to build. After loading the data set, the code had to re-create all the words from the pre- processed data set to build an index, which translates all lists of word-indices to strings and then used Term Frequence - Inverse Document Frequency (TF-IDF) as text representation. TF-IDF is a statistical measure used to evaluate how important a word is in a document. First, it computes the Term Frequence (TF) for each review, the Inverse Document Frequency (IDF) using each review and finally, the TF-IDF for each review. It transforms on the Test data which computes the TF for each review, then the TF-IDF for each review using the IDF from the Training data. Finally, the model was fit to classify the sentiment of the movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b33c52-b3f0-41aa-8a4a-199358214072",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d8a94d-7cd9-4094-b2f3-441160daa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb672ca8-5003-401a-8060-55bde0ca8ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of the dataset is :  74858\n"
     ]
    }
   ],
   "source": [
    "# Setting up the tokenizer\n",
    "max_words = 10000 #max vocabulary size\n",
    "tokenizer = Tokenizer(num_words=max_words,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_imdb)\n",
    "print(\"Vocabulary of the dataset is : \",len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d16f6c6-b7b6-4b0b-990d-ccf3abd058ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences of reviews\n",
    "seq_train_imdb = tokenizer.texts_to_sequences(X_train_imdb)\n",
    "seq_test_imdb =  tokenizer.texts_to_sequences(X_test_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0272ef3-59cc-496c-8496-6da2961c3076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data padded!\n"
     ]
    }
   ],
   "source": [
    "# Truncate and pad input sequences\n",
    "max_review_length = 500 #input length for embedding layer \n",
    "X_train_pad_imdb=pad_sequences(seq_train_imdb,truncating = 'post',\n",
    "                               padding = 'pre',maxlen=max_review_length)\n",
    "X_test_pad_imdb=pad_sequences(seq_test_imdb,truncating = 'post', \n",
    "                              padding = 'pre',maxlen=max_review_length)\n",
    "print(\"Data padded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644a6fa-c3f3-486d-b0ea-a88632f798d3",
   "metadata": {},
   "source": [
    "### Training and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d824c06-67ff-4b7c-b576-efc066e5683e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training data: \n",
      "(29750, 500)\n",
      "Size of Validation data: \n",
      "(5250, 500)\n",
      "Size of Testing data: \n",
      "(15000, 500)\n"
     ]
    }
   ],
   "source": [
    "X_train_pad_imdb, X_val_pad_imdb, y_train_imdb, y_val_imdb = train_test_split(X_train_pad_imdb, y_train_imdb, \n",
    "                                                   test_size = 0.15, \n",
    "                                                   random_state = 42)\n",
    "\n",
    "print(\"Size of Training data: \")\n",
    "print(X_train_pad_imdb.shape)\n",
    "print(\"Size of Validation data: \")\n",
    "print(X_val_pad_imdb.shape)\n",
    "print(\"Size of Testing data: \")\n",
    "print(X_test_pad_imdb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e5e8f1-0181-4255-8a0b-3c6b3579b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac785707-4536-4647-a867-a1d2775b6b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 16:14:37.588631: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 32)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 80)                36160     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356,241\n",
      "Trainable params: 356,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##### MODEL #####\n",
    "# Create the model\n",
    "embedding_vector_length = 32 #output dimension \n",
    "epochs = 3\n",
    "batch_size = 256\n",
    "dropout_rate = 0.1\n",
    "lstm_units = 80\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(max_words,embedding_vector_length, input_length = max_review_length))\n",
    "lstm.add(Dropout(dropout_rate))\n",
    "lstm.add(LSTM(lstm_units)) \n",
    "lstm.add(Dropout(dropout_rate))\n",
    "lstm.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = \"Adam\" \n",
    "lstm.compile(loss = \"binary_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "print(lstm.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768858c7-1d84-4ea4-baca-6075aa5ab3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "117/117 [==============================] - 99s 835ms/step - loss: 0.6046 - accuracy: 0.7101 - val_loss: 0.4487 - val_accuracy: 0.8349\n",
      "Epoch 2/3\n",
      "117/117 [==============================] - 93s 795ms/step - loss: 0.3103 - accuracy: 0.8731 - val_loss: 0.2856 - val_accuracy: 0.8804\n",
      "Epoch 3/3\n",
      "117/117 [==============================] - 93s 792ms/step - loss: 0.2198 - accuracy: 0.9171 - val_loss: 0.2792 - val_accuracy: 0.8874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8836b17fd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "lstm.fit(X_train_pad_imdb, y_train_imdb,\n",
    "          epochs = epochs, \n",
    "          batch_size = batch_size,\n",
    "          validation_data = (X_val_pad_imdb, y_val_imdb)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62c8dcb9-c09e-4101-aa4d-145aa06a634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 23s 49ms/step - loss: 0.2869 - accuracy: 0.8811\n",
      "Accuracy: 88.11%\n"
     ]
    }
   ],
   "source": [
    "scores_lstm = lstm.evaluate(X_test_pad_imdb,y_test_imdb)\n",
    "print (\"Accuracy: %.2f%%\" %(scores_lstm[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2935c",
   "metadata": {},
   "source": [
    "#### Dataset: Tweets\n",
    "\n",
    "##### Deep Learning\n",
    "\n",
    "##### Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8164b2-5f7a-4d1a-929b-d7f32990115c",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "- benchmark ranking of classifiers on different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39d02f-9855-4f7c-9a9c-d97ebb32394c",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- valuable insights on method selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e9fec",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363afd8a",
   "metadata": {},
   "source": [
    "Gaye, B., Zhang, D., & Wulamu, A. (2021). A Tweet Sentiment Classification Approach Using a Hybrid Stacked Ensemble Technique. Information, 12(9), 374.\n",
    "\n",
    "Alsayat, A. (2021). Improving Sentiment Analysis for Social Media Applications Using an Ensemble Deep Learning Language Model. Arabian Journal for Science and Engineering, 1-13.\n",
    "\n",
    "Vizcarra, J., Kozaki, K., Ruiz, M. T., & Quintero, R. (2021). Knowledge-based sentiment analysis and visualization on social networks. New Generation Computing, 39(1), 199-229.\n",
    "\n",
    "B. Pang, L. Lee, S. Vaithyanathan. (2002). Thumbs up? Sentiment Classification using\n",
    "Machine Learning Techniques. Proceedings of EMNLP 2002. pp. 79-86.\n",
    "\n",
    "Q. Ain, M. Ali, A. Riaz, A. Noureen, M. Kamran, B. Hayat, A. Rehman. (2017). Sentiment\n",
    "Analysis using Deep Learning techniques. (IJACSA) International Journal of Advanced\n",
    "Computer Science and Applications, Vol. 8, No. 6.\n",
    "\n",
    "B. Pang, L. Lee. (2008). Opinion Mining and Sentiment Analysis. Foundations and Trends in\n",
    "Information Retrieval Vol. 2, Nos. 1–2. DOI: 10.1561/1500000001. pp. 1-135.\n",
    "\n",
    "P. Ekman, W. Friesen, P. Ellsworth. (1982). What emotion categories or dimensions can\n",
    "observers judge from facial behavior?. Emotion in the human face. Cambridge University\n",
    "Press, New York. pp 39-55.\n",
    "\n",
    "M. Kurosu. (2015). Human-Computer Interaction. 17th International Conference, HCI\n",
    "International 2015, Los Angeles. Proceedings, Part II. p. 423. [book]\n",
    "\n",
    "X. Wang, Y. Liu, C. Sun, B. Wang, X. Wang. (2015). Predicting Polarities of Tweets by\n",
    "Composing Word Embeddings with Long Short-Term Memory. Proceedings of the 53rd\n",
    "Annual Meeting of the Association for Computational Linguistics and the 7th International\n",
    "Joint Conference on Natural Language Processing. pp. 1343-1353.\n",
    "\n",
    "X. Wang, C. Zhang, Y. Ji, L. Sun, L. Wu, Z. Bao. (2013). A Depression Detection Model\n",
    "Based on Sentiment Analysis in Micro-blog Social Network. PAKDD 2013: Trends and\n",
    "Applications in Knowledge Discovery and Data Mining. pp. 201-213.\n",
    "\n",
    "R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuglu, P. Kuksa. (2011). Natural\n",
    "Language Processing (Almost) from Scratch. Journal of Machine Learning Research 12. pp.\n",
    "2493-2537.\n",
    "\n",
    "Bacco, L., Cimino, A., Dell’Orletta, F., & Merone, M. (2021). Explainable Sentiment Analysis: A Hierarchical Transformer-Based Extractive Summarization Approach. Electronics, 10(18), 2195.\n",
    "\n",
    "Wu, Z., Ying, C., Dai, X., Huang, S., & Chen, J. (2020, October). Transformer-Based Multi-aspect Modeling for Multi-aspect Multi-sentiment Analysis. In CCF International Conference on Natural Language Processing and Chinese Computing (pp. 546-557). Springer, Cham.\n",
    "\n",
    "Jiang, M., Wu, J., Shi, X., & Zhang, M. (2019). Transformer based memory network for sentiment analysis of web comments. IEEE Access, 7, 179942-179953.\n",
    "\n",
    "Novikova, A., & Stupnikov, S. (2017, July). Sentiment analysis of short texts from social networks using sentiment lexicons and blending of machine learning algorithms. In Proc. CEUR Workshop (pp. 190-201).\n",
    "\n",
    "Singh, J., Singh, G., & Singh, R. (2017). Optimization of sentiment analysis using machine learning classifiers. Human-centric Computing and information Sciences, 7(1), 1-12.\n",
    "\n",
    "Rustam, F., Ashraf, I., Mehmood, A., Ullah, S., & Choi, G. S. (2019). Tweets classification on the base of sentiments for US airline companies. Entropy, 21(11), 1078.\n",
    "\n",
    "Ahuja, R., Chug, A., Kohli, S., Gupta, S., & Ahuja, P. (2019). The impact of features extraction on the sentiment analysis. Procedia Computer Science, 152, 341-348.\n",
    "\n",
    "PURCHASES, C. J. O. I., STOYANOVA, L., & WALLACE, W. (2019). TOPIC MODELLING, SENTIMENT ANALSYS AND CLASSIFICATION OF SHORT-FORM TEXT.\n",
    "\n",
    "da Silva, N. F. F., Coletta, L. F., Hruschka, E. R., & Hruschka Jr, E. R. (2016). Using unsupervised information to improve semi-supervised tweet sentiment classification. Information Sciences, 355, 348-365.\n",
    "\n",
    "Araque, O., Corcuera-Platas, I., Sánchez-Rada, J. F., & Iglesias, C. A. (2017). Enhancing deep learning sentiment analysis with ensemble techniques in social applications. Expert Systems with Applications, 77, 236-246.\n",
    "\n",
    "Yi, S., & Liu, X. (2020). Machine learning based customer sentiment analysis for recommending shoppers, shops based on customers’ review. Complex & Intelligent Systems, 6(3), 621-634.\n",
    "\n",
    "Ouyang, X., Zhou, P., Li, C. H., & Liu, L. (2015, October). Sentiment analysis using convolutional neural network. In 2015 IEEE international conference on computer and information technology; ubiquitous computing and communications; dependable, autonomic and secure computing; pervasive intelligence and computing (pp. 2359-2364). IEEE.\n",
    "\n",
    "Jiang, M., Wu, J., Shi, X., & Zhang, M. (2019). Transformer based memory network for sentiment analysis of web comments. IEEE Access, 7, 179942-179953.\n",
    "\n",
    "Wu, Z., Ying, C., Dai, X., Huang, S., & Chen, J. (2020, October). Transformer-Based Multi-aspect Modeling for Multi-aspect Multi-sentiment Analysis. In CCF International Conference on Natural Language Processing and Chinese Computing (pp. 546-557). Springer, Cham.\n",
    "\n",
    "Elfaik, H. (2021). Deep Bidirectional LSTM Network Learning-Based Sentiment Analysis for Arabic Text. Journal of Intelligent Systems, 30(1), 395-412.\n",
    "\n",
    "Alexandridis, G., Varlamis, I., Korovesis, K., Caridakis, G., & Tsantilas, P. (2021). A Survey on Sentiment Analysis and Opinion Mining in Greek Social Media. Information, 12(8), 331.\n",
    "\n",
    "Flender, M., & Gips, C. (2017, September). Sentiment Analysis of a German Twitter-Corpus. In LWDA (p. 25).\n",
    "\n",
    "Tellez, E. S., Miranda-Jiménez, S., Graff, M., Moctezuma, D., Siordia, O. S., & Villaseñor, E. A. (2017). A case study of Spanish text transformations for twitter sentiment analysis. Expert Systems with Applications, 81, 457-471.\n",
    "\n",
    "Carosia, A. E. O., Coelho, G. P., & Silva, A. E. A. (2020). Analyzing the Brazilian financial market through Portuguese sentiment analysis in social media. Applied Artificial Intelligence, 34(1), 1-19."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
